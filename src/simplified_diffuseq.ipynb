{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model for Open Dialogue\n",
    "\n",
    "Simplified version\n",
    "\n",
    "References\n",
    "- DiffuSeq (cited below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "\n",
    "[1] @inproceedings{gong2022diffuseq,\n",
    "  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  booktitle = {International Conference on Learning Representations, ICLR},\n",
    "  title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},\n",
    "  year = 2023\n",
    "}\n",
    "\n",
    "[2] @article{gong2023diffuseqv2,\n",
    "  title={DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models},\n",
    "  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  journal={arXiv preprint arXiv:2310.05793},\n",
    "  year={2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adapt from other codes\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Use Commonsense Conversation dataset (from Reddit)\n",
    "\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note that, in DiffuSeq, a model file is created to store all training progress, configuration etc. (in bash format poitning to raw files?)\n",
    "\n",
    "- denoise rate ?\n",
    "- using updates in v2 diffuseq took it from 2 days -> 11 hr learning time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiffuSeq [1]\n",
    "12 layers of Transformer with 12 attention\n",
    "heads, where the time step embedding is plugged akin to the position embedding. \n",
    "The\n",
    "maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2;000\n",
    "and a square-root noise schedule. To reduce the out-of-vocabulary generation, we apply Byte Pair\n",
    "Encoding (Sennrich et al., 2016) to construct the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding dimension = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "# hidden size of time embedding\n",
    "hidden_dim = 128 \n",
    "\n",
    "# :param seq_len: the max sequence length (one-side).\n",
    "seq_len = 128 \n",
    "\n",
    "# TODO good value for this\n",
    "output_dims = 128\n",
    "\n",
    "# Same as diffuSeq\n",
    "num_diffusion_timesteps = 2000\n",
    "\n",
    "lr=1e-04\n",
    "\n",
    "# TODO figure out what are the right params to recreate diffuSeq\n",
    "batch_size = 20 \n",
    "lr = 0.001 # learning rate\n",
    "ema_rate = 0.999\n",
    "weight_decay = 0.01\n",
    "learning_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# use GPU if available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer & embeddings\n",
    "Get tokenizer from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "class myTokenizer():\n",
    "    \"\"\"\n",
    "    Load tokenizer from bert config \n",
    "    \"\"\"\n",
    "    ################################################\n",
    "    ### You can custome your own tokenizer here. ###\n",
    "    ################################################\n",
    "    def __init__(self, args):\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token_id = tokenizer.sep_token_id\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        # TODO save\n",
    "        # tokenizer.save_pretrained(args.checkpoint_path)\n",
    "                \n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "        # args.vocab_size = self.vocab_size # update vocab size in args\n",
    "    \n",
    "    def encode_token(self, sentences):\n",
    "        input_ids = self.tokenizer(sentences, add_special_tokens=True)['input_ids']\n",
    "        return input_ids\n",
    "        \n",
    "    def decode_token(self, seq):\n",
    "        seq = seq.squeeze(-1).tolist()\n",
    "        while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "            seq.pop()\n",
    "        tokens = self.tokenizer.decode(seq)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallykalumba/dev/diffusion-dialogue/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "# TODO have actual args\n",
    "# args = {\"vocab_size\": 0}\n",
    "tokenizer = myTokenizer(args)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding function $EMB(w)$ to map the discrete text $w$ into a continuous space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8799, -1.0476, -0.5496,  ..., -0.0408,  1.3313,  0.2616],\n",
       "        [-0.3120,  0.2555, -0.0969,  ...,  0.0627, -0.3481,  1.4801],\n",
       "        [ 1.3775,  0.6762,  0.6025,  ...,  0.6886,  2.2067, -0.1571],\n",
       "        ...,\n",
       "        [ 1.3659, -0.0026, -1.2989,  ...,  0.0580,  0.0269,  0.3280],\n",
       "        [-1.0132,  0.7414,  0.4947,  ..., -0.8917,  0.5757, -1.0895],\n",
       "        [ 1.1187, -1.0189,  1.5840,  ...,  0.8912, -0.3734,  0.3581]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = torch.nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
    "\n",
    "# initialize random embeddings\n",
    "torch.nn.init.normal_(model_emb.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 128)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample text data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data in training data json file \n",
    "# TODO do this in a different way \n",
    "# TODO load actual dataset from Amazon\n",
    "\n",
    "import json\n",
    "\n",
    "data_dir = \"./datasets/sample\"\n",
    "path = f'{data_dir}/train.jsonl'\n",
    "\n",
    "sentence_lst = {'src':[], 'trg': []}\n",
    "with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "# TODO use pandas to load faster? any other package can just load json directly rather than row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is my favorite story arc . ca n't wait to see how he does in the tourney ! the show is my guarantee smile for the week .\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lst['src'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src', 'trg'],\n",
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "raw_datasets = Dataset.from_dict(sentence_lst)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': \"this is my favorite story arc . ca n't wait to see how he does in the tourney ! the show is my guarantee smile for the week .\",\n",
       " 'trg': \"yea it 's hard not to have a smile on your face the entire episode\"}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 19/19 [00:00<00:00, 19.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "        input_id_x = tokenizer.encode_token(examples['src'])\n",
    "        input_id_y = tokenizer.encode_token(examples['trg'])\n",
    "        result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "        return result_dict\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_id_x', 'input_id_y'],\n",
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"input_id_x\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to collate the batch\n",
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    mask_ = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merge and mask: 100%|██████████| 19/19 [00:00<00:00, 675.01 examples/s]\n",
      "padding: 100%|██████████| 19/19 [00:00<00:00, 668.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def merge_and_mask(group_lst):\n",
    "        lst = []\n",
    "        mask = []\n",
    "        for i in range(len(group_lst['input_id_x'])):\n",
    "            end_token = group_lst['input_id_x'][i][-1]\n",
    "            src = group_lst['input_id_x'][i][:-1]\n",
    "            trg = group_lst['input_id_y'][i][:-1]\n",
    "            while len(src) + len(trg) > seq_len - 3:\n",
    "                if len(src)>len(trg):\n",
    "                    src.pop()\n",
    "                elif len(src)<len(trg):\n",
    "                    trg.pop()\n",
    "                else:\n",
    "                    src.pop()\n",
    "                    trg.pop()\n",
    "            src.append(end_token)\n",
    "            trg.append(end_token)\n",
    "\n",
    "            lst.append(src + [tokenizer.sep_token_id] + trg)\n",
    "            mask.append([0]*(len(src)+1))\n",
    "        group_lst['input_ids'] = lst\n",
    "        group_lst['input_mask'] = mask\n",
    "        return group_lst\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "        merge_and_mask,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"merge and mask\",\n",
    "    )\n",
    "    \n",
    "def pad_function(group_lst):\n",
    "    max_length = seq_len\n",
    "    group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], tokenizer.pad_token_id, max_length)\n",
    "    group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "    return group_lst\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "        pad_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"padding\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 19\n",
      "}) padded dataset\n"
     ]
    }
   ],
   "source": [
    "print(lm_datasets, 'padded dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "raw_datasets = datasets.DatasetDict()\n",
    "raw_datasets['train'] = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into iterable data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch as th\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets\n",
    "        self.length = len(self.text_datasets['train'])\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE we need to be able to decode the tokens back to text space, function is available for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "train_dataset = TextDataset(raw_datasets, model_emb=model_emb)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "data = iter(data_loader)\n",
    "\n",
    "# NOTE don't use next here since we only have 1 batch to use\n",
    "# next(data)\n",
    "# TODO load data for the validation, inference differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE next() will get the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.70015603, -0.5362382 , -0.17453256, ...,  0.42478082,\n",
       "         -1.2100865 , -1.0228456 ],\n",
       "        [ 0.96171385,  0.0127369 ,  0.30993783, ..., -0.5008502 ,\n",
       "         -1.0706804 ,  1.5289253 ],\n",
       "        [ 0.69762117, -0.14255983,  0.19132029, ..., -0.41612247,\n",
       "          0.6816389 , -0.5276403 ],\n",
       "        ...,\n",
       "        [ 0.8799339 , -1.0476243 , -0.54961586, ..., -0.0408201 ,\n",
       "          1.3312643 ,  0.26159418],\n",
       "        [ 0.8799339 , -1.0476243 , -0.54961586, ..., -0.0408201 ,\n",
       "          1.3312643 ,  0.26159418],\n",
       "        [ 0.8799339 , -1.0476243 , -0.54961586, ..., -0.0408201 ,\n",
       "          1.3312643 ,  0.26159418]], dtype=float32),\n",
       " {'input_ids': array([  101,  2023,  2003,  2026,  5440,  2466,  8115,  1012,  6187,\n",
       "          1050,  1005,  1056,  3524,  2000,  2156,  2129,  2002,  2515,\n",
       "          1999,  1996,  2778,  5420,   999,  1996,  2265,  2003,  2026,\n",
       "         11302,  2868,  2005,  1996,  2733,  1012,   102,   102,   101,\n",
       "          6300,  2050,  2009,  1005,  1055,  2524,  2025,  2000,  2031,\n",
       "          1037,  2868,  2006,  2115,  2227,  1996,  2972,  2792,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       "  'input_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model = TransformerNetModel(\n",
    "#         input_dims=hidden_dim,\n",
    "#         output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "#         hidden_t_dim=hidden_t_dim,\n",
    "#         dropout=dropout,\n",
    "#         config_name=config_name,\n",
    "#         vocab_size=vocab_size,\n",
    "#         init_pretrained=use_plm_init\n",
    "#     )\n",
    "\n",
    "#     betas = gd.get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "\n",
    "#     if not timestep_respacing:\n",
    "#         timestep_respacing = [diffusion_steps]\n",
    "\n",
    "#     diffusion = SpacedDiffusion(\n",
    "#         use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "#         betas=betas,\n",
    "#         rescale_timesteps=rescale_timesteps,\n",
    "#         predict_xstart=predict_xstart,\n",
    "#         learn_sigmas = learn_sigma,\n",
    "#         sigma_small = sigma_small,\n",
    "#         use_kl = use_kl,\n",
    "#         rescale_learned_sigmas=rescale_learned_sigmas\n",
    "#     )\n",
    "\n",
    "# FIXME need to implement a way to save the model progress so that\n",
    "# trained model can be loaded later for assessment purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch as th\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = th.exp(\n",
    "        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n",
    "    )#.to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
    "\n",
    "\n",
    "    print(\"N x dim Tensor of positional embeddings\", embedding)\n",
    "    print(\"Size of embedding\", embedding.size())\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME need to define TransformerNetModel\n",
    "#  The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "# Adapted from diffuSeq\n",
    "\n",
    "# TODO code the transformer from scratch\n",
    "# TODO design the transformer from scratch\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dims, hidden_t_dim, output_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        # FIXME set to an actual value\n",
    "        config.hidden_dropout_prob = 0\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        self.output_dims = output_dims\n",
    "        # self.dropout = dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        print(\"transformer self.hidden_size\", self.hidden_size)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "        # Generate logits for hidden representation\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "\n",
    "        # Time embeddings\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            # params as input features * output features\n",
    "            nn.Linear(hidden_t_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Function to deal with having a hidden size not equal to input size, project to hidden size (?)\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            # NOTE input_dims = 128\n",
    "            # hidden_size = 768\n",
    "            # self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "            #                                 nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "            # FIXME this is trying to convert to hidden size 768 why??? it's already in the hidden siz \n",
    "            # FIXME this actually doesn't seeem necessary to do????\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(config.hidden_size, input_dims),\n",
    "                                             nn.Tanh(), nn.Linear(input_dims, input_dims))\n",
    "\n",
    "        # FIXME why is this temporary \n",
    "        temp_bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "       \n",
    "       # FIXME why do we do this 2 times????\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "        # self.lm_head.weight.requires_grad = False\n",
    "        # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "        # TODO explain what is happening\n",
    "        self.input_transformers = temp_bert.encoder\n",
    "        # TODO explain what is doing\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "        self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "     \n",
    "        del temp_bert.embeddings\n",
    "        del temp_bert.pooler\n",
    "\n",
    "        # FIXME When does this get used\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # FIXME what is happening here\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "    \n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.word_embedding(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_repr):\n",
    "    # NOTE we make a simplifying assumption get the logits from linear layer\n",
    "        return self.lm_head(hidden_repr)\n",
    "                \n",
    "\n",
    "    # FIXME what is the difference btween BertModel, BertConfig, BertTokenizer, maybe define it all in one place config for tokenizer + embeddings?\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        # FIXME update string comment\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        print(\"Forward step\")\n",
    "        \n",
    "        # print(\"timesteps: a 1-D batch of timesteps:\",timesteps)\n",
    "        # print(\"self.hidden_t_dim\", self.hidden_t_dim)\n",
    "\n",
    "        # Embedded timestep\n",
    "\n",
    "        # print(\"1D batch of timesteps\",timesteps,timesteps.size())\n",
    "        \n",
    "        # Note timestep_embedding returns an N * dim Tensor of positional embeddings\n",
    "        # Note it gives N*128 embeddings\n",
    "\n",
    "        # expects a 1D tensor as input, with the size of the input tensor being (hidden_t_dim,)\n",
    "        emb_t = self.time_embed(timestep_embedding(timesteps, self.hidden_t_dim))\n",
    "\n",
    "        print(\"x: an [N x C x ...] Tensor of inputs\",\"size:\",x.size())\n",
    "        # NOTE x is already of size 19x128x768, so don't need to expand\n",
    "        # FIXME why doesn't this work as expected?\n",
    "        # if self.input_dims != self.hidden_size:\n",
    "        #     # FIXME change this to convert to 19x128x768 to ...x128\n",
    "        #     emb_x = self.input_up_proj(x)\n",
    "        # else:\n",
    "        # FIXME do we want this to get pushed to the correct size?\n",
    "        emb_x = x\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, : seq_length ]\n",
    "        print(\"emb_x.shape, emb_t.shape, self.position_embeddings\")\n",
    "        print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
    "        emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "\n",
    "        input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "\n",
    "        print(\"input_trans_hideden_states.shape\", input_trans_hidden_states.shape)\n",
    "        \n",
    "        # if self.output_dims != self.hidden_size:\n",
    "        #     # FIXME this should allow the output to be projected down to 19x128x128 so that it's osame as input data\n",
    "        #     h = self.output_down_proj(input_trans_hidden_states)\n",
    "        #     print(\"transformed h.shape\", h.shape)\n",
    "        # else:\n",
    "        # FIXME why this transofmration not required, it seems the model output compared for mse calculation is also having end dimension x768\n",
    "        h = input_trans_hidden_states\n",
    "        h = h.type(x.dtype)\n",
    "\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explore other simplistic sample code\n",
    "# https://github.com/lucidrains/denoising-diffusion-pytorch\n",
    "# https://e-dorigatti.github.io/math/deep%20learning/2023/06/25/diffusion.html\n",
    "# https://github.com/tanelp/tiny-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE adapted from diffuSeq, which is adapted from https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "class GaussianDiffusion():\n",
    "    def __init__(\n",
    "        self,\n",
    "        betas): \n",
    "\n",
    "        # TODO consider adding these other parameters, where are they used, are they required?\n",
    "    #     predict_xstart,\n",
    "    #     rescale_learned_sigmas,\n",
    "    #     learn_sigmas,\n",
    "    #     sigma_small,\n",
    "    #     use_kl,\n",
    "    #     rescale_timesteps=False,\n",
    "    # ):\n",
    "    #     self.rescale_timesteps = rescale_timesteps\n",
    "    #     self.predict_xstart = predict_xstart\n",
    "    #     self.rescale_learned_sigmas = rescale_learned_sigmas\n",
    "    #     self.learn_sigmas = learn_sigmas\n",
    "    #     self.sigma_small = sigma_small\n",
    "    #     self.use_kl = use_kl\n",
    "\n",
    "    #    :param rescale_timesteps: if True, pass floating point timesteps into the\n",
    "    #                           model so that they are always scaled like in the\n",
    "    #                           original paper (0 to 1000).\n",
    "        # Assume true\n",
    "        self.rescale_timesteps = True\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        # FIXME what is alphas? why?\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        # FIXME copied directly\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # log calculation clipped because the posterior variance is 0 at the\n",
    "        # beginning of the diffusion chain.\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    # NOTE the below comments from diffuSeq\n",
    "    # self.mapping_func = None # implement in train main()\n",
    "    # self.add_mask_noise = False # TODO\n",
    "\n",
    "    # FIXME copied directly from diffuSeq\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "    \n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "    \n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "        # FIXME why this isn't working\n",
    "        # if mask == None:\n",
    "        #     return x_t\n",
    "        # else:\n",
    "        mask = th.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape)\n",
    "        return th.where(mask==0, x_start, x_t)\n",
    "        \n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.size(0), x.size(-1)\n",
    "        assert t.shape == (B,)\n",
    "        # print(x.shape)\n",
    "        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
    "        \n",
    "        # for fixedlarge, we set the initial (log-)variance like so\n",
    "        # to get a better decoder log likelihood.\n",
    "        model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n",
    "        model_log_variance = np.log(np.append(self.posterior_variance[1], self.betas[1:]))\n",
    "        \n",
    "        model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                # print(denoised_fn)\n",
    "                x = denoised_fn(x, t)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        # if self.predict_xstart:\n",
    "        #     pred_xstart = process_xstart(model_output)\n",
    "        # else:\n",
    "            ### model is used to predict eps\n",
    "        pred_xstart = process_xstart(\n",
    "            self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "        )\n",
    "\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "            top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = th.randn_like(x)\n",
    "            replace_mask = th.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = th.randn_like(noise[replace_mask])\n",
    "                replace_mask = th.abs(noise) > top_p\n",
    "            assert (th.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = th.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"],\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    \n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            if not clamp_first:\n",
    "                if i > clamp_step:\n",
    "                    denoised_fn_cur = None\n",
    "                else:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "            else:\n",
    "                if i >= clamp_step:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    denoised_fn_cur = None\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = th.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        # print(x_start_mean.device, noise.device)\n",
    "        return (\n",
    "             x_start_mean + std * noise\n",
    "        )\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "        # print(logits.shape)\n",
    "        loss_fct = th.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll *= mask\n",
    "        # print(decoder_nll.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1)/mask.sum(dim=-1)\n",
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1)\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t):\n",
    "\n",
    "        # if self.predict_xstart:\n",
    "        #     pred_xstart = model_output\n",
    "        #     pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "        #         x_start=pred_xstart, x_t=x, t=t\n",
    "        #     )\n",
    "\n",
    "        # else: # predict eps\n",
    "        pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "    \n",
    "        pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        return {'pred_xprev':pred_prev, 'pred_xstart':pred_xstart}\n",
    "\n",
    "    def training_losses_seq2seq(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs. # not used unless fixing the input embeddings\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        x_start_fix = x_start # save the orignal x_0\n",
    "\n",
    "        # Note this is size 19 x 128 x 128 \n",
    "        print(\"x_start size\", x_start.size())\n",
    "\n",
    "        assert 'input_ids' in model_kwargs\n",
    "        input_ids_x = model_kwargs.pop('input_ids')#.to(t.device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask')#.to(t.device)\n",
    "        x_start_mean = model.get_embeds(input_ids_x)\n",
    "\n",
    "        # 12 x 128 x 768 once you get the embeds \n",
    "        print(\"x_start_mean size\", x_start_mean.size())\n",
    "        \n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   th.tensor([0]).to(x_start_mean.device),\n",
    "                                   x_start_mean.shape)\n",
    "        # print(std.shape, )\n",
    "        # x_start_log_var = 2 * th.log(std)\n",
    "        x_start = self._get_x_start(x_start_mean, std)\n",
    "        # print(x_start_mean.shape, x_start.shape)\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "\n",
    "        # Diffuse the data in x_start (return a noisy version)\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask) # reparametrization trick.\n",
    "\n",
    "        get_logits = model.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
    "        # FIXME try to fix dimensionality error\n",
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "        # FIXME why these aren't the same shape, why is this asserted when the conversion shows moving to hidden dimension\n",
    "        # assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart'] # predicted_xstart = model_output\n",
    "        t0_mask = (t == 0)\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2)\n",
    "        terms[\"mse\"] = th.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "\n",
    "        # tT_mask = (t == self.num_timesteps - 1)\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, th.LongTensor([self.num_timesteps - 1]).to(x_start.device))\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x) # embedding regularization\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t) # x_0->model_out_x_start\n",
    "        # assert (model.lm_head.weight == model.word_embedding.weight).all()\n",
    "\n",
    "        terms[\"loss\"] = terms[\"mse\"] + decoder_nll + tT_loss\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = th.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n",
    "            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # print(sigma.mean())\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        if langevin_fn:\n",
    "            print(t.shape)\n",
    "            sample=langevin_fn(sample, mean_pred, sigma, self.alphas_cumprod_prev[t[0]], t, x)\n",
    "        \n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_reverse_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n",
    "            - out[\"pred_xstart\"]\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n",
    "\n",
    "        # Equation 12. reversed\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n",
    "            + th.sqrt(1 - alpha_bar_next) * eps\n",
    "        )\n",
    "\n",
    "        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler():\n",
    "    \"\"\"\n",
    "    A distribution over timesteps in the diffusion process, intended to reduce\n",
    "    variance of the objective.\n",
    "\n",
    "    Sampler performs unbiased importance sampling, in which the\n",
    "    objective's mean is unchanged.\n",
    "    TODO confirm & update comment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diffusion):\n",
    "        self.diffusion = diffusion\n",
    "        self._weights = np.ones([diffusion.num_timesteps])\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Importance-sample timesteps for a batch.\n",
    "\n",
    "        :param batch_size: the number of timesteps.\n",
    "        :param device: the torch device to save to.\n",
    "        :return: a tuple (timesteps, weights):\n",
    "                 - timesteps: a tensor of timestep indices.\n",
    "                 - weights: a tensor of weights to scale the resulting losses.\n",
    "        \"\"\"\n",
    "        w = self.weights()\n",
    "        p = w / np.sum(w)\n",
    "        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n",
    "        indices = th.from_numpy(indices_np).long()#.to(device)\n",
    "        weights_np = 1 / (len(p) * p[indices_np])\n",
    "        weights = th.from_numpy(weights_np).float()#.to(device)\n",
    "        return indices, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training loop\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    \"\"\"\n",
    "    Update target parameters to be closer to those of source parameters using\n",
    "    an exponential moving average.\n",
    "\n",
    "    :param target_params: the target parameter sequence.\n",
    "    :param source_params: the source parameter sequence.\n",
    "    :param rate: the EMA rate (closer to 1 means slower).\n",
    "    \"\"\"\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n",
    "\n",
    "def zero_grad(model_params):\n",
    "    for param in model_params:\n",
    "        # Taken from https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer.add_param_group\n",
    "        if param.grad is not None:\n",
    "            param.grad.detach_()\n",
    "            param.grad.zero_()\n",
    "\n",
    "def log_loss_dict(diffusion, ts, losses):\n",
    "    for key, values in losses.items():\n",
    "        # logger.logkv_mean(key, values.mean().item())\n",
    "        # Log the quantiles (four quartiles, in particular).\n",
    "        for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
    "            quartile = int(4 * sub_t / diffusion.num_timesteps)\n",
    "            # logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import copy\n",
    "\n",
    "class TrainLoop():\n",
    "    def __init__(\n",
    "        self,\n",
    "    #     *,\n",
    "        model,\n",
    "        diffusion,\n",
    "        data,\n",
    "        batch_size,\n",
    "    #     microbatch,\n",
    "        lr,\n",
    "        ema_rate,\n",
    "    #     log_interval,\n",
    "        #  schedule_sampler=None,\n",
    "        weight_decay=0.0,\n",
    "        learning_steps=0,\n",
    "    #     checkpoint_path='',\n",
    "    #     gradient_clipping=-1.,\n",
    "        eval_data=None,\n",
    "        eval_interval=-1,\n",
    "    ):\n",
    "          self.model = model\n",
    "          self.ddp_model = model # NOTE no distribution training\n",
    "          self.diffusion = diffusion\n",
    "          self.data = data\n",
    "          self.batch_size = batch_size\n",
    "        #   self.microbatch = microbatch if microbatch > 0 else batch_size\n",
    "          # Assume no microbatch\n",
    "          self.microbatch = batch_size\n",
    "\n",
    "          self.lr = lr\n",
    "          self.ema_rate = (\n",
    "            [ema_rate]\n",
    "            if isinstance(ema_rate, float)\n",
    "            else [float(x) for x in ema_rate.split(\",\")]\n",
    "            )\n",
    "          \n",
    "          # NOTE assuming uniform sampler\n",
    "          self.schedule_sampler = UniformSampler(diffusion)\n",
    "          self.weight_decay = weight_decay\n",
    "          self.learning_steps = learning_steps\n",
    "          self.eval_data = eval_data\n",
    "          self.eval_interval = eval_interval\n",
    "          \n",
    "          self.step = 0\n",
    "\n",
    "          # TODO check other initialization steps are covered\n",
    "\n",
    "          self.model_params = list(self.model.parameters())\n",
    "          self.master_params = self.model_params\n",
    "\n",
    "          # Optimizer\n",
    "          self.opt = AdamW(self.master_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "          self.ema_params = [\n",
    "                copy.deepcopy(self.master_params) for _ in range(len(self.ema_rate))\n",
    "            ]\n",
    "\n",
    "    def _log_grad_norm(self):\n",
    "        sqsum = 0.0\n",
    "        for p in self.master_params:\n",
    "            if p.grad != None:\n",
    "                sqsum += (p.grad ** 2).sum().item()\n",
    "        # TODO implement logging\n",
    "        # logger.logkv_mean(\"grad_norm\", np.sqrt(sqsum))\n",
    "\n",
    "    def _anneal_lr(self):\n",
    "        if not self.learning_steps:\n",
    "            return\n",
    "        frac_done = (self.step) / self.learning_steps\n",
    "        lr = self.lr * (1 - frac_done)\n",
    "        for param_group in self.opt.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    \n",
    "    def optimize_normal(self):\n",
    "        # NOTE assuming no gradient clipping\n",
    "        self._log_grad_norm()\n",
    "        self._anneal_lr()\n",
    "        self.opt.step()\n",
    "        for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "            update_ema(params, self.master_params, rate=rate)\n",
    "     \n",
    "\n",
    "    def run_step(self, batch, cond):\n",
    "        # TODO implement this fn\n",
    "        self.forward_backward(batch, cond)\n",
    "        # NOTE assuming not using fp16 optimization\n",
    "        self.optimize_normal()\n",
    "        # TODO do this fn - logging\n",
    "        # self.log_step()\n",
    "\n",
    "    def forward_only(self, batch, cond):\n",
    "        with th.no_grad():\n",
    "            zero_grad(self.model_params)\n",
    "            for i in range(0, batch.shape[0], self.microbatch):\n",
    "                micro = batch[i: i + self.microbatch]#.to(device)\n",
    "                micro_cond = {\n",
    "                    k: v[i: i + self.microbatch]#.to(device)\n",
    "                    for k, v in cond.items()\n",
    "                }\n",
    "                last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "                t, weights = self.schedule_sampler.sample(micro.shape[0], device)\n",
    "\n",
    "                compute_losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "\n",
    "                # NOTE not using ddp - distributed training\n",
    "                with self.ddp_model.no_sync():\n",
    "                        losses = compute_losses()\n",
    "\n",
    "                log_loss_dict(\n",
    "                    self.diffusion, t, {f\"eval_{k}\": v * weights for k, v in losses.items()}\n",
    "                )\n",
    "\n",
    "\n",
    "    def forward_backward(self, batch, cond):\n",
    "        print(\"batch size\", batch.size())\n",
    "        zero_grad(self.model_params)\n",
    "        for i in range(0, batch.shape[0], self.microbatch):\n",
    "            micro = batch[i : i + self.microbatch] #.to(dist_util.dev())\n",
    "            micro_cond = {\n",
    "                k: v[i : i + self.microbatch]#.to(dist_util.dev())\n",
    "                for k, v in cond.items()\n",
    "            }\n",
    "            last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "            t, weights = self.schedule_sampler.sample(micro.shape[0],torch.device)# dist_util.dev())\n",
    "            # print(micro_cond.keys())\n",
    "            losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "\n",
    "            # NOTE not using ddp - distributed training\n",
    "            # with self.ddp_model.no_sync():\n",
    "\n",
    "            # NOTE losses datatype a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                #  Some mean or variance settings may also have other keys.\n",
    "            # losses = compute_losses()\n",
    "\n",
    "            print(\"losses\", losses)\n",
    "\n",
    "            # if isinstance(self.schedule_sampler, LossAwareSampler):\n",
    "            #     self.schedule_sampler.update_with_local_losses(\n",
    "            #         t, losses[\"loss\"].detach()\n",
    "            #     )\n",
    "\n",
    "            loss = (losses[\"loss\"] * weights).mean()\n",
    "            log_loss_dict(\n",
    "                self.diffusion, t, {k: v * weights for k, v in losses.items()}\n",
    "            )\n",
    "            # NOTE not using fp16 optimization \n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "    # NOTE not going to implement capacity to resume training     \n",
    "    # NOTE removed saving checkpoints \n",
    "    # NOTE removed some logging\n",
    "    def run_loop(self):\n",
    "        while (\n",
    "            # FIXME check these defined correctly\n",
    "            not self.learning_steps\n",
    "            or self.step < self.learning_steps\n",
    "        ):\n",
    "            # NOTE check the loop runs\n",
    "            print(\"About to run next(self.data)\")\n",
    "            batch, cond = next(self.data)\n",
    "            print(\"Running loop!\")\n",
    "            # TODO add this fn\n",
    "            self.run_step(batch, cond)\n",
    "            if self.eval_data is not None and self.step % self.eval_interval == 0:\n",
    "                batch_eval, cond_eval = next(self.eval_data)\n",
    "                # TODO add this fn\n",
    "                self.forward_only(batch_eval, cond_eval)\n",
    "                print('eval on validation set')\n",
    "            self.step += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate all the classes for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer self.hidden_size 768\n",
      "### The parameter count is 110184634\n"
     ]
    }
   ],
   "source": [
    "# Define the noise schedule\n",
    "# TODO consider other noise schedules, here take the simplifying assumption that we use linear noise schedule \n",
    "# Linear schedule from Ho et al, extended to work for any number of\n",
    "# diffusion steps.\n",
    "\n",
    "scale = 1000 / num_diffusion_timesteps\n",
    "beta_start = scale * 0.0001\n",
    "beta_end = scale * 0.02\n",
    "betas = np.linspace(\n",
    "    beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    ")\n",
    "\n",
    "# NOTE we assume NO timestep respacing \n",
    "# TODO instantiate the diffusion model & transformer and save it (?)\n",
    "\n",
    "diffusion = GaussianDiffusion(betas=betas)\n",
    "\n",
    "# TODO specify correct dimensions!\n",
    "# Note embedding size is 128\n",
    "model = TransformerNetModel(vocab_size=vocab_size, input_dims=embedding_dim, hidden_t_dim=hidden_dim, output_dims=output_dims)\n",
    "\n",
    "# model.to(device) \n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "# TODO add to logger\n",
    "print(f'### The parameter count is {pytorch_total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for training\n",
    "\n",
    "Note the implementation details in DiffuSeq (first version) is\n",
    "\"The maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2000\n",
    "and a square-root noise schedule.\"\n",
    "\n",
    "How is it different in v2 or other papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to run next(self.data)\n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Run training loop\n",
    "\n",
    "try:\n",
    "    TrainLoop(\n",
    "            model=model,\n",
    "            diffusion=diffusion,\n",
    "            data=data,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr,\n",
    "            ema_rate=ema_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            learning_steps=learning_steps,\n",
    "        ).run_loop()\n",
    "\n",
    "except StopIteration as e:\n",
    "    print(e)\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached StopIteration Exception. Training completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sample output using the forward step of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Inference step (sampling / generation part) -->\n",
    "# Decoding\n",
    "\n",
    "Once the training completed, we can start the inference step and get cross validation accuracy.\n",
    "\n",
    "Not using diffuseq 2 (DPM Solver++ method), but original method.\n",
    "In this study we do not consider the time or space complexity of the inference step.\n",
    "\n",
    "(Adapted from sample_seq2seq.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO set up saving of the trained model & load it from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE normally need to load tokenizer & model_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.myTokenizer at 0x12eea93c0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 128)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE normally need to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficient_knn(model_emb, text_emb):\n",
    "    emb_norm = (model_emb**2).sum(-1).view(-1, 1) # vocab\n",
    "    text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1) # d, bsz*seqlen\n",
    "    arr_norm = (text_emb ** 2).sum(-1).view(-1, 1) # bsz*seqlen, 1\n",
    "    # print(emb_norm.shape, arr_norm.shape)\n",
    "    dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(model_emb, text_emb_t) # (vocab, d) x (d, bsz*seqlen)\n",
    "    dist = torch.clamp(dist, 0.0, np.inf)\n",
    "    # print(dist.shape)\n",
    "    topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "    return topk_out.values, topk_out.indices\n",
    "\n",
    "def denoised_fn_round(model, text_emb):\n",
    "    # print(text_emb.shape) # bsz, seqlen, dim\n",
    "    model_emb = model.weight  # input_embs\n",
    "    # print(t)\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "    # val, indices = get_knn(model_emb, text_emb.to(model_emb.device), dist=dist)\n",
    "    val, indices = get_efficient_knn(model_emb, text_emb.to(model_emb.device))\n",
    "    rounded_tokens = indices[0]\n",
    "    # print(rounded_tokens.shape)\n",
    "    new_embeds = model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### End of reading iteration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.8900,  0.0225, -0.2468,  ...,  0.1533,  0.1329,  0.1152],\n",
      "        [ 0.8900,  0.0225, -0.2468,  ...,  0.1533,  0.1329,  0.1152],\n",
      "        [ 0.8900,  0.0225, -0.2468,  ...,  0.1533,  0.1329,  0.1152],\n",
      "        ...,\n",
      "        [ 0.8900,  0.0225, -0.2468,  ...,  0.1533,  0.1329,  0.1152],\n",
      "        [ 0.8900,  0.0225, -0.2468,  ...,  0.1533,  0.1329,  0.1152],\n",
      "        [ 0.8900,  0.0225, -0.2468,  ...,  0.1533,  0.1329,  0.1152]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.9996, -0.3990,  0.1252,  ...,  0.1532,  0.1328,  0.1151],\n",
      "        [ 0.9996, -0.3990,  0.1252,  ...,  0.1532,  0.1328,  0.1151],\n",
      "        [ 0.9996, -0.3990,  0.1252,  ...,  0.1532,  0.1328,  0.1151],\n",
      "        ...,\n",
      "        [ 0.9996, -0.3990,  0.1252,  ...,  0.1532,  0.1328,  0.1151],\n",
      "        [ 0.9996, -0.3990,  0.1252,  ...,  0.1532,  0.1328,  0.1151],\n",
      "        [ 0.9996, -0.3990,  0.1252,  ...,  0.1532,  0.1328,  0.1151]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.8646, -0.7469,  0.4799,  ...,  0.1532,  0.1328,  0.1150],\n",
      "        [ 0.8646, -0.7469,  0.4799,  ...,  0.1532,  0.1328,  0.1150],\n",
      "        [ 0.8646, -0.7469,  0.4799,  ...,  0.1532,  0.1328,  0.1150],\n",
      "        ...,\n",
      "        [ 0.8646, -0.7469,  0.4799,  ...,  0.1532,  0.1328,  0.1150],\n",
      "        [ 0.8646, -0.7469,  0.4799,  ...,  0.1532,  0.1328,  0.1150],\n",
      "        [ 0.8646, -0.7469,  0.4799,  ...,  0.1532,  0.1328,  0.1150]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.5178, -0.9570,  0.7678,  ...,  0.1531,  0.1327,  0.1150],\n",
      "        [ 0.5178, -0.9570,  0.7678,  ...,  0.1531,  0.1327,  0.1150],\n",
      "        [ 0.5178, -0.9570,  0.7678,  ...,  0.1531,  0.1327,  0.1150],\n",
      "        ...,\n",
      "        [ 0.5178, -0.9570,  0.7678,  ...,  0.1531,  0.1327,  0.1150],\n",
      "        [ 0.5178, -0.9570,  0.7678,  ...,  0.1531,  0.1327,  0.1150],\n",
      "        [ 0.5178, -0.9570,  0.7678,  ...,  0.1531,  0.1327,  0.1150]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.0443, -0.9904,  0.9491,  ...,  0.1530,  0.1326,  0.1149],\n",
      "        [ 0.0443, -0.9904,  0.9491,  ...,  0.1530,  0.1326,  0.1149],\n",
      "        [ 0.0443, -0.9904,  0.9491,  ...,  0.1530,  0.1326,  0.1149],\n",
      "        ...,\n",
      "        [ 0.0443, -0.9904,  0.9491,  ...,  0.1530,  0.1326,  0.1149],\n",
      "        [ 0.0443, -0.9904,  0.9491,  ...,  0.1530,  0.1326,  0.1149],\n",
      "        [ 0.0443, -0.9904,  0.9491,  ...,  0.1530,  0.1326,  0.1149]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.4401, -0.8410,  0.9985,  ...,  0.1529,  0.1326,  0.1149],\n",
      "        [-0.4401, -0.8410,  0.9985,  ...,  0.1529,  0.1326,  0.1149],\n",
      "        [-0.4401, -0.8410,  0.9985,  ...,  0.1529,  0.1326,  0.1149],\n",
      "        ...,\n",
      "        [-0.4401, -0.8410,  0.9985,  ...,  0.1529,  0.1326,  0.1149],\n",
      "        [-0.4401, -0.8410,  0.9985,  ...,  0.1529,  0.1326,  0.1149],\n",
      "        [-0.4401, -0.8410,  0.9985,  ...,  0.1529,  0.1326,  0.1149]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.8167, -0.5365,  0.9092,  ...,  0.1529,  0.1325,  0.1148],\n",
      "        [-0.8167, -0.5365,  0.9092,  ...,  0.1529,  0.1325,  0.1148],\n",
      "        [-0.8167, -0.5365,  0.9092,  ...,  0.1529,  0.1325,  0.1148],\n",
      "        ...,\n",
      "        [-0.8167, -0.5365,  0.9092,  ...,  0.1529,  0.1325,  0.1148],\n",
      "        [-0.8167, -0.5365,  0.9092,  ...,  0.1529,  0.1325,  0.1148],\n",
      "        [-0.8167, -0.5365,  0.9092,  ...,  0.1529,  0.1325,  0.1148]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.9934, -0.1328,  0.6935,  ...,  0.1528,  0.1324,  0.1148],\n",
      "        [-0.9934, -0.1328,  0.6935,  ...,  0.1528,  0.1324,  0.1148],\n",
      "        [-0.9934, -0.1328,  0.6935,  ...,  0.1528,  0.1324,  0.1148],\n",
      "        ...,\n",
      "        [-0.9934, -0.1328,  0.6935,  ...,  0.1528,  0.1324,  0.1148],\n",
      "        [-0.9934, -0.1328,  0.6935,  ...,  0.1528,  0.1324,  0.1148],\n",
      "        [-0.9934, -0.1328,  0.6935,  ...,  0.1528,  0.1324,  0.1148]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.9268,  0.2953,  0.3815,  ...,  0.1527,  0.1324,  0.1147],\n",
      "        [-0.9268,  0.2953,  0.3815,  ...,  0.1527,  0.1324,  0.1147],\n",
      "        [-0.9268,  0.2953,  0.3815,  ...,  0.1527,  0.1324,  0.1147],\n",
      "        ...,\n",
      "        [-0.9268,  0.2953,  0.3815,  ...,  0.1527,  0.1324,  0.1147],\n",
      "        [-0.9268,  0.2953,  0.3815,  ...,  0.1527,  0.1324,  0.1147],\n",
      "        [-0.9268,  0.2953,  0.3815,  ...,  0.1527,  0.1324,  0.1147]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.6334,  0.6689,  0.0165,  ...,  0.1526,  0.1323,  0.1146],\n",
      "        [-0.6334,  0.6689,  0.0165,  ...,  0.1526,  0.1323,  0.1146],\n",
      "        [-0.6334,  0.6689,  0.0165,  ...,  0.1526,  0.1323,  0.1146],\n",
      "        ...,\n",
      "        [-0.6334,  0.6689,  0.0165,  ...,  0.1526,  0.1323,  0.1146],\n",
      "        [-0.6334,  0.6689,  0.0165,  ...,  0.1526,  0.1323,  0.1146],\n",
      "        [-0.6334,  0.6689,  0.0165,  ...,  0.1526,  0.1323,  0.1146]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.1849,  0.9191, -0.3509,  ...,  0.1525,  0.1322,  0.1146],\n",
      "        [-0.1849,  0.9191, -0.3509,  ...,  0.1525,  0.1322,  0.1146],\n",
      "        [-0.1849,  0.9191, -0.3509,  ...,  0.1525,  0.1322,  0.1146],\n",
      "        ...,\n",
      "        [-0.1849,  0.9191, -0.3509,  ...,  0.1525,  0.1322,  0.1146],\n",
      "        [-0.1849,  0.9191, -0.3509,  ...,  0.1525,  0.1322,  0.1146],\n",
      "        [-0.1849,  0.9191, -0.3509,  ...,  0.1525,  0.1322,  0.1146]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.3089,  0.9996, -0.6694,  ...,  0.1525,  0.1322,  0.1145],\n",
      "        [ 0.3089,  0.9996, -0.6694,  ...,  0.1525,  0.1322,  0.1145],\n",
      "        [ 0.3089,  0.9996, -0.6694,  ...,  0.1525,  0.1322,  0.1145],\n",
      "        ...,\n",
      "        [ 0.3089,  0.9996, -0.6694,  ...,  0.1525,  0.1322,  0.1145],\n",
      "        [ 0.3089,  0.9996, -0.6694,  ...,  0.1525,  0.1322,  0.1145],\n",
      "        [ 0.3089,  0.9996, -0.6694,  ...,  0.1525,  0.1322,  0.1145]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.7271,  0.8957, -0.8950,  ...,  0.1524,  0.1321,  0.1145],\n",
      "        [ 0.7271,  0.8957, -0.8950,  ...,  0.1524,  0.1321,  0.1145],\n",
      "        [ 0.7271,  0.8957, -0.8950,  ...,  0.1524,  0.1321,  0.1145],\n",
      "        ...,\n",
      "        [ 0.7271,  0.8957, -0.8950,  ...,  0.1524,  0.1321,  0.1145],\n",
      "        [ 0.7271,  0.8957, -0.8950,  ...,  0.1524,  0.1321,  0.1145],\n",
      "        [ 0.7271,  0.8957, -0.8950,  ...,  0.1524,  0.1321,  0.1145]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.9672,  0.6264, -0.9962,  ...,  0.1523,  0.1320,  0.1144],\n",
      "        [ 0.9672,  0.6264, -0.9962,  ...,  0.1523,  0.1320,  0.1144],\n",
      "        [ 0.9672,  0.6264, -0.9962,  ...,  0.1523,  0.1320,  0.1144],\n",
      "        ...,\n",
      "        [ 0.9672,  0.6264, -0.9962,  ...,  0.1523,  0.1320,  0.1144],\n",
      "        [ 0.9672,  0.6264, -0.9962,  ...,  0.1523,  0.1320,  0.1144],\n",
      "        [ 0.9672,  0.6264, -0.9962,  ...,  0.1523,  0.1320,  0.1144]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.9706,  0.2415, -0.9590,  ...,  0.1522,  0.1320,  0.1144],\n",
      "        [ 0.9706,  0.2415, -0.9590,  ...,  0.1522,  0.1320,  0.1144],\n",
      "        [ 0.9706,  0.2415, -0.9590,  ...,  0.1522,  0.1320,  0.1144],\n",
      "        ...,\n",
      "        [ 0.9706,  0.2415, -0.9590,  ...,  0.1522,  0.1320,  0.1144],\n",
      "        [ 0.9706,  0.2415, -0.9590,  ...,  0.1522,  0.1320,  0.1144],\n",
      "        [ 0.9706,  0.2415, -0.9590,  ...,  0.1522,  0.1320,  0.1144]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.7363, -0.1879, -0.7885,  ...,  0.1522,  0.1319,  0.1143],\n",
      "        [ 0.7363, -0.1879, -0.7885,  ...,  0.1522,  0.1319,  0.1143],\n",
      "        [ 0.7363, -0.1879, -0.7885,  ...,  0.1522,  0.1319,  0.1143],\n",
      "        ...,\n",
      "        [ 0.7363, -0.1879, -0.7885,  ...,  0.1522,  0.1319,  0.1143],\n",
      "        [ 0.7363, -0.1879, -0.7885,  ...,  0.1522,  0.1319,  0.1143],\n",
      "        [ 0.7363, -0.1879, -0.7885,  ...,  0.1522,  0.1319,  0.1143]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.3217, -0.5827, -0.5085,  ...,  0.1521,  0.1318,  0.1142],\n",
      "        [ 0.3217, -0.5827, -0.5085,  ...,  0.1521,  0.1318,  0.1142],\n",
      "        [ 0.3217, -0.5827, -0.5085,  ...,  0.1521,  0.1318,  0.1142],\n",
      "        ...,\n",
      "        [ 0.3217, -0.5827, -0.5085,  ...,  0.1521,  0.1318,  0.1142],\n",
      "        [ 0.3217, -0.5827, -0.5085,  ...,  0.1521,  0.1318,  0.1142],\n",
      "        [ 0.3217, -0.5827, -0.5085,  ...,  0.1521,  0.1318,  0.1142]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.1716, -0.8699, -0.1578,  ...,  0.1520,  0.1318,  0.1142],\n",
      "        [-0.1716, -0.8699, -0.1578,  ...,  0.1520,  0.1318,  0.1142],\n",
      "        [-0.1716, -0.8699, -0.1578,  ...,  0.1520,  0.1318,  0.1142],\n",
      "        ...,\n",
      "        [-0.1716, -0.8699, -0.1578,  ...,  0.1520,  0.1318,  0.1142],\n",
      "        [-0.1716, -0.8699, -0.1578,  ...,  0.1520,  0.1318,  0.1142],\n",
      "        [-0.1716, -0.8699, -0.1578,  ...,  0.1520,  0.1318,  0.1142]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.6229, -0.9966,  0.2148,  ...,  0.1519,  0.1317,  0.1141],\n",
      "        [-0.6229, -0.9966,  0.2148,  ...,  0.1519,  0.1317,  0.1141],\n",
      "        [-0.6229, -0.9966,  0.2148,  ...,  0.1519,  0.1317,  0.1141],\n",
      "        ...,\n",
      "        [-0.6229, -0.9966,  0.2148,  ...,  0.1519,  0.1317,  0.1141],\n",
      "        [-0.6229, -0.9966,  0.2148,  ...,  0.1519,  0.1317,  0.1141],\n",
      "        [-0.6229, -0.9966,  0.2148,  ...,  0.1519,  0.1317,  0.1141]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.9217, -0.9393,  0.5575,  ...,  0.1519,  0.1316,  0.1141],\n",
      "        [-0.9217, -0.9393,  0.5575,  ...,  0.1519,  0.1316,  0.1141],\n",
      "        [-0.9217, -0.9393,  0.5575,  ...,  0.1519,  0.1316,  0.1141],\n",
      "        ...,\n",
      "        [-0.9217, -0.9393,  0.5575,  ...,  0.1519,  0.1316,  0.1141],\n",
      "        [-0.9217, -0.9393,  0.5575,  ...,  0.1519,  0.1316,  0.1141],\n",
      "        [-0.9217, -0.9393,  0.5575,  ...,  0.1519,  0.1316,  0.1141]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.9948, -0.7087,  0.8228,  ...,  0.1518,  0.1316,  0.1140],\n",
      "        [-0.9948, -0.7087,  0.8228,  ...,  0.1518,  0.1316,  0.1140],\n",
      "        [-0.9948, -0.7087,  0.8228,  ...,  0.1518,  0.1316,  0.1140],\n",
      "        ...,\n",
      "        [-0.9948, -0.7087,  0.8228,  ...,  0.1518,  0.1316,  0.1140],\n",
      "        [-0.9948, -0.7087,  0.8228,  ...,  0.1518,  0.1316,  0.1140],\n",
      "        [-0.9948, -0.7087,  0.8228,  ...,  0.1518,  0.1316,  0.1140]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.8244, -0.3473,  0.9738,  ...,  0.1517,  0.1315,  0.1140],\n",
      "        [-0.8244, -0.3473,  0.9738,  ...,  0.1517,  0.1315,  0.1140],\n",
      "        [-0.8244, -0.3473,  0.9738,  ...,  0.1517,  0.1315,  0.1140],\n",
      "        ...,\n",
      "        [-0.8244, -0.3473,  0.9738,  ...,  0.1517,  0.1315,  0.1140],\n",
      "        [-0.8244, -0.3473,  0.9738,  ...,  0.1517,  0.1315,  0.1140],\n",
      "        [-0.8244, -0.3473,  0.9738,  ...,  0.1517,  0.1315,  0.1140]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[-0.4521,  0.0782,  0.9894,  ...,  0.1516,  0.1314,  0.1139],\n",
      "        [-0.4521,  0.0782,  0.9894,  ...,  0.1516,  0.1314,  0.1139],\n",
      "        [-0.4521,  0.0782,  0.9894,  ...,  0.1516,  0.1314,  0.1139],\n",
      "        ...,\n",
      "        [-0.4521,  0.0782,  0.9894,  ...,  0.1516,  0.1314,  0.1139],\n",
      "        [-0.4521,  0.0782,  0.9894,  ...,  0.1516,  0.1314,  0.1139],\n",
      "        [-0.4521,  0.0782,  0.9894,  ...,  0.1516,  0.1314,  0.1139]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[0.0309, 0.4893, 0.8676,  ..., 0.1516, 0.1314, 0.1138],\n",
      "        [0.0309, 0.4893, 0.8676,  ..., 0.1516, 0.1314, 0.1138],\n",
      "        [0.0309, 0.4893, 0.8676,  ..., 0.1516, 0.1314, 0.1138],\n",
      "        ...,\n",
      "        [0.0309, 0.4893, 0.8676,  ..., 0.1516, 0.1314, 0.1138],\n",
      "        [0.0309, 0.4893, 0.8676,  ..., 0.1516, 0.1314, 0.1138],\n",
      "        [0.0309, 0.4893, 0.8676,  ..., 0.1516, 0.1314, 0.1138]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[0.5063, 0.8101, 0.6252,  ..., 0.1515, 0.1313, 0.1138],\n",
      "        [0.5063, 0.8101, 0.6252,  ..., 0.1515, 0.1313, 0.1138],\n",
      "        [0.5063, 0.8101, 0.6252,  ..., 0.1515, 0.1313, 0.1138],\n",
      "        ...,\n",
      "        [0.5063, 0.8101, 0.6252,  ..., 0.1515, 0.1313, 0.1138],\n",
      "        [0.5063, 0.8101, 0.6252,  ..., 0.1515, 0.1313, 0.1138],\n",
      "        [0.5063, 0.8101, 0.6252,  ..., 0.1515, 0.1313, 0.1138]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[0.8578, 0.9813, 0.2960,  ..., 0.1514, 0.1312, 0.1137],\n",
      "        [0.8578, 0.9813, 0.2960,  ..., 0.1514, 0.1312, 0.1137],\n",
      "        [0.8578, 0.9813, 0.2960,  ..., 0.1514, 0.1312, 0.1137],\n",
      "        ...,\n",
      "        [0.8578, 0.9813, 0.2960,  ..., 0.1514, 0.1312, 0.1137],\n",
      "        [0.8578, 0.9813, 0.2960,  ..., 0.1514, 0.1312, 0.1137],\n",
      "        [0.8578, 0.9813, 0.2960,  ..., 0.1514, 0.1312, 0.1137]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.9992,  0.9715, -0.0744,  ...,  0.1513,  0.1312,  0.1137],\n",
      "        [ 0.9992,  0.9715, -0.0744,  ...,  0.1513,  0.1312,  0.1137],\n",
      "        [ 0.9992,  0.9715, -0.0744,  ...,  0.1513,  0.1312,  0.1137],\n",
      "        ...,\n",
      "        [ 0.9992,  0.9715, -0.0744,  ...,  0.1513,  0.1312,  0.1137],\n",
      "        [ 0.9992,  0.9715, -0.0744,  ...,  0.1513,  0.1312,  0.1137],\n",
      "        [ 0.9992,  0.9715, -0.0744,  ...,  0.1513,  0.1312,  0.1137]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.8960,  0.7823, -0.4344,  ...,  0.1513,  0.1311,  0.1136],\n",
      "        [ 0.8960,  0.7823, -0.4344,  ...,  0.1513,  0.1311,  0.1136],\n",
      "        [ 0.8960,  0.7823, -0.4344,  ...,  0.1513,  0.1311,  0.1136],\n",
      "        ...,\n",
      "        [ 0.8960,  0.7823, -0.4344,  ...,  0.1513,  0.1311,  0.1136],\n",
      "        [ 0.8960,  0.7823, -0.4344,  ...,  0.1513,  0.1311,  0.1136],\n",
      "        [ 0.8960,  0.7823, -0.4344,  ...,  0.1513,  0.1311,  0.1136]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.5734,  0.4488, -0.7341,  ...,  0.1512,  0.1310,  0.1136],\n",
      "        [ 0.5734,  0.4488, -0.7341,  ...,  0.1512,  0.1310,  0.1136],\n",
      "        [ 0.5734,  0.4488, -0.7341,  ...,  0.1512,  0.1310,  0.1136],\n",
      "        ...,\n",
      "        [ 0.5734,  0.4488, -0.7341,  ...,  0.1512,  0.1310,  0.1136],\n",
      "        [ 0.5734,  0.4488, -0.7341,  ...,  0.1512,  0.1310,  0.1136],\n",
      "        [ 0.5734,  0.4488, -0.7341,  ...,  0.1512,  0.1310,  0.1136]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "N x dim Tensor of positional embeddings tensor([[ 0.1105,  0.0324, -0.9318,  ...,  0.1511,  0.1310,  0.1135],\n",
      "        [ 0.1105,  0.0324, -0.9318,  ...,  0.1511,  0.1310,  0.1135],\n",
      "        [ 0.1105,  0.0324, -0.9318,  ...,  0.1511,  0.1310,  0.1135],\n",
      "        ...,\n",
      "        [ 0.1105,  0.0324, -0.9318,  ...,  0.1511,  0.1310,  0.1135],\n",
      "        [ 0.1105,  0.0324, -0.9318,  ...,  0.1511,  0.1310,  0.1135],\n",
      "        [ 0.1105,  0.0324, -0.9318,  ...,  0.1511,  0.1310,  0.1135]])\n",
      "Size of embedding torch.Size([19, 128])\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "# FIXME just this time load the same data\n",
    "data_valid = iter(data_loader)\n",
    "\n",
    "all_test_data = []\n",
    "\n",
    "# NOTE can do a handling for separate nodes of GPU, just loading in one go here\n",
    "try:\n",
    "    while True:\n",
    "        batch, cond = next(data_valid)\n",
    "        # print(batch.shape)\n",
    "        all_test_data.append(cond)\n",
    "\n",
    "except StopIteration:\n",
    "    print('### End of reading iteration...')\n",
    "\n",
    "from tqdm import tqdm\n",
    "iterator = tqdm(all_test_data)\n",
    "\n",
    "# iterator = iter(all_test_data)\n",
    "\n",
    "for cond in iterator:\n",
    "    input_ids_x = cond.pop('input_ids')#.to(dist_util.dev())\n",
    "    x_start = model.get_embeds(input_ids_x)\n",
    "    input_ids_mask = cond.pop('input_mask')\n",
    "    input_ids_mask_ori = input_ids_mask\n",
    "\n",
    "    noise = th.randn_like(x_start)\n",
    "    input_ids_mask = th.broadcast_to(input_ids_mask.unsqueeze(dim=-1), x_start.shape)#.to(dist_util.dev())\n",
    "    x_noised = th.where(input_ids_mask == 0, x_start, noise)\n",
    "\n",
    "\n",
    "    # step argument\n",
    "    # if less than diffusion training steps, like 1000, use ddim sampling\n",
    "    # NOTE we have diffusion_steps = 2000, just use that for simplificity\n",
    "\n",
    "    # if args.step == args.diffusion_steps:\n",
    "    use_ddim = False\n",
    "    step_gap = 1\n",
    "\n",
    "    sample_fn = diffusion.p_sample_loop\n",
    "\n",
    "\n",
    "    # args.seq_len = seq_len \n",
    "    # args.hidden_dim = hidden_dim\n",
    "    sample_shape = (x_start.shape[0], seq_len, hidden_dim)\n",
    "\n",
    "    class Args:\n",
    "        seq_len = seq_len\n",
    "        hidden_dim = hidden_dim\n",
    "        use_ddim = False \n",
    "\n",
    "    args = Args()\n",
    "\n",
    "\n",
    "    samples = sample_fn(\n",
    "            model,\n",
    "            sample_shape,\n",
    "            noise=x_noised,\n",
    "            # clip_denoised=args.clip_denoised, takes default=True\n",
    "            # TODO check that params given to denoised_fn_round are correct\n",
    "            # partial(denoised_fn_round, args, model_emb)\n",
    "            # denoised_fn: if not None, a function which applies to the\n",
    "            # x_start prediction before it is used to sample.\n",
    "            # FIXME is this function required?\n",
    "            # denoised_fn=denoised_fn_round(model=model_emb, text_emb=model_emb),\n",
    "            model_kwargs={},\n",
    "            # top_p=args.top_p, -> top p used in sampling, default is off\n",
    "            clamp_step=1700, #args.clamp_step,\n",
    "            clamp_first=True, # clamp first mode\n",
    "            mask=input_ids_mask,\n",
    "            x_start=x_start,\n",
    "            gap=step_gap\n",
    "        )\n",
    "    \n",
    "    sample = samples[-1]\n",
    "\n",
    "    print('decoding for seq2seq', )\n",
    "    print(sample.shape)\n",
    "\n",
    "    logits = model.get_logits(sample)  # bsz, seqlen, vocab\n",
    "    cands = th.topk(logits, k=1, dim=-1)\n",
    "\n",
    "    word_lst_recover = []\n",
    "    word_lst_ref = []\n",
    "    word_lst_source = []\n",
    "\n",
    "    for seq, input_mask in zip(cands.indices, input_ids_mask_ori):\n",
    "        len_x = args.seq_len - sum(input_mask).tolist()\n",
    "        tokens = tokenizer.decode_token(seq[len_x:])\n",
    "        word_lst_recover.append(tokens)\n",
    "\n",
    "    for seq, input_mask in zip(input_ids_x, input_ids_mask_ori):\n",
    "        # tokens = tokenizer.decode_token(seq)\n",
    "        len_x = args.seq_len - sum(input_mask).tolist()\n",
    "        word_lst_source.append(tokenizer.decode_token(seq[:len_x]))\n",
    "        word_lst_ref.append(tokenizer.decode_token(seq[len_x:]))\n",
    "\n",
    "    # for i in range(world_size):\n",
    "    #     if i == rank:  # Write files sequentially\n",
    "    #         fout = open(out_path, 'a')\n",
    "    #         for (recov, ref, src) in zip(word_lst_recover, word_lst_ref, word_lst_source):\n",
    "    #             print(json.dumps({\"recover\": recov, \"reference\": ref, \"source\": src}), file=fout)\n",
    "    #         fout.close()\n",
    "    #     dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover\n",
    "word_lst_source\n",
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if i == rank:  # Write files sequentially\n",
    "out_path = \"test.txt\"\n",
    "fout = open(out_path, 'a')\n",
    "for (recov, ref, src) in zip(word_lst_recover, word_lst_ref, word_lst_source):\n",
    "    print(json.dumps({\"recover\": recov, \"reference\": ref, \"source\": src}), file=fout)\n",
    "fout.close()\n",
    "            # dist.barrier()\n",
    "\n",
    "    # print('### Total takes {:.2f}s .....'.format(time.time() - start_t))\n",
    "print(f'### Written the decoded output to {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
