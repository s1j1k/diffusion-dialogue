{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d15281",
   "metadata": {},
   "source": [
    "# Diffusion Model for Open Dialogue\n",
    "\n",
    "Simplified version\n",
    "\n",
    "References\n",
    "- DiffuSeq (cited below)\n",
    "\n",
    "Adapted from:\n",
    "\n",
    "[1] @inproceedings{gong2022diffuseq,\n",
    "  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  booktitle = {International Conference on Learning Representations, ICLR},\n",
    "  title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},\n",
    "  year = 2023\n",
    "}\n",
    "\n",
    "[2] @article{gong2023diffuseqv2,\n",
    "  title={DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models},\n",
    "  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  journal={arXiv preprint arXiv:2310.05793},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- Use Commonsense Conversation dataset (from Reddit)\n",
    "\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- Use Commonsense Conversation dataset (from Reddit)\n",
    "\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n",
    "## Training\n",
    "\n",
    "Note that, in DiffuSeq, a model file is created to store all training progress, configuration etc. (in bash format poitning to raw files?)\n",
    "\n",
    "- denoise rate ?\n",
    "- using updates in v2 diffuseq took it from 2 days -> 11 hr learning time\n",
    "\n",
    "Set parameters\n",
    "\n",
    "DiffuSeq [1]\n",
    "12 layers of Transformer with 12 attention\n",
    "heads, where the time step embedding is plugged akin to the position embedding. \n",
    "The\n",
    "maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2;000\n",
    "and a square-root noise schedule. To reduce the out-of-vocabulary generation, we apply Byte Pair\n",
    "Encoding (Sennrich et al., 2016) to construct the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0342105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yun\\anaconda3\\envs\\AI4E\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import torch as th\n",
    "from datasets import Dataset, DatasetDict\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772d123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "# choose embedding dimension = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "# hidden size of time embedding\n",
    "hidden_dim = 128 \n",
    "\n",
    "# :param seq_len: the max sequence length (one-side).\n",
    "seq_len = 128 \n",
    "\n",
    "# TODO good value for this\n",
    "output_dims = 128\n",
    "\n",
    "# Same as diffuSeq\n",
    "num_diffusion_timesteps = 500\n",
    "\n",
    "lr=1e-04\n",
    "\n",
    "# TODO figure out what are the right params to recreate diffuSeq\n",
    "batch_size = 10\n",
    "lr = 0.001 # learning rate\n",
    "ema_rate = 0.999\n",
    "weight_decay = 0.01\n",
    "learning_steps = 1000\n",
    "\n",
    "\n",
    "# use GPU if available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "    \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182b8c4",
   "metadata": {},
   "source": [
    "Get tokenizer from BERT <br>\n",
    "Embedding function $EMB(w)$ to map the discrete text $w$ into a continuous space. <br>\n",
    "Load the sample text data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c8c905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = torch.nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
    "\n",
    "# initialize random embeddings\n",
    "torch.nn.init.normal_(model_emb.weight)\n",
    "\n",
    "model_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d906f014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Samples: 3382137\n",
      "Validation Data Samples: 2048\n",
      "Test Data Samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# read in the data in training data json file \n",
    "# TODO do this in a different way \n",
    "# TODO load actual dataset from Amazon\n",
    "\n",
    "# data_dir = \"./datasets/sample\"\n",
    "# path = f'{data_dir}/train.jsonl'\n",
    "\n",
    "# sentence_lst = {'src':[], 'trg': []}\n",
    "# with open(path, 'r') as f_reader:\n",
    "#         for row in f_reader:\n",
    "#             content = json.loads(row)\n",
    "#             sentence_lst['src'].append(content['src'].strip())\n",
    "#             sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "# TODO use pandas to load faster? any other package can just load json directly rather than row by row\n",
    "\n",
    "data_dir = \"./datasets\"\n",
    "train_path = f'{data_dir}/train_full.jsonl'\n",
    "valid_path = f'{data_dir}/valid_full.jsonl'\n",
    "test_path = f'{data_dir}/test_full.jsonl'\n",
    "\n",
    "def load_data(path, limit=None):\n",
    "    sentence_lst = {'src':[], 'trg': []}\n",
    "    with open(path, 'r') as f_reader:\n",
    "        for i, row in enumerate(f_reader):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "    return sentence_lst\n",
    "\n",
    "# Load datasets with size restriction\n",
    "train_limit = None  # Limit the size of the training set\n",
    "valid_limit = None   # Limit the size of the validation set\n",
    "test_limit = None    # Limit the size of the test set\n",
    "\n",
    "train_data = load_data(train_path, limit=train_limit)\n",
    "valid_data = load_data(valid_path, limit=valid_limit)\n",
    "test_data = load_data(test_path, limit=test_limit)\n",
    "\n",
    "print(\"Training Data Samples:\", len(train_data['src']))\n",
    "print(\"Validation Data Samples:\", len(valid_data['src']))\n",
    "print(\"Test Data Samples:\", len(test_data['src']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87f573e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': 'jesus , what kind of concerts do you go to where people sucker punch you for being born tall ?',\n",
       " 'trg': 'the kind that allow bitter short people in . so basically all of them .'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['src'][0]\n",
    "raw_datasets = Dataset.from_dict(train_data)\n",
    "raw_datasets\n",
    "raw_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset (num_proc=4):  30%|██████▎              | 1016000/3382137 [02:58<06:07, 6442.28 examples/s]"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    input_id_x = tokenizer(examples['src'], add_special_tokens=True)['input_ids']\n",
    "    input_id_y = tokenizer(examples['trg'], add_special_tokens=True)['input_ids']\n",
    "    result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "    return result_dict\n",
    "\n",
    "# Use partial to pass the tokenizer to the tokenize_function\n",
    "tokenize_function_with_tokenizer = partial(tokenize_function, tokenizer=tokenizer)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "valid_dataset = Dataset.from_dict(valid_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing training dataset\",\n",
    ")\n",
    "\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    tokenize_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing validation dataset\",\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing test dataset\",\n",
    ")\n",
    "\n",
    "# Combine into DatasetDict\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_valid_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n",
    "print(\"Tokenization complete.\")\n",
    "print(\"Training Set:\", len(tokenized_datasets['train']))\n",
    "print(\"Validation Set:\", len(tokenized_datasets['validation']))\n",
    "print(\"Test Set:\", len(tokenized_datasets['test']))\n",
    "\n",
    "# tokenized_datasets\n",
    "# len(tokenized_datasets['train'][\"input_id_x\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e56578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge and mask sequences\n",
    "def merge_and_mask(group_lst):\n",
    "    lst = []\n",
    "    mask = []\n",
    "    for i in range(len(group_lst['input_id_x'])):\n",
    "        end_token = group_lst['input_id_x'][i][-1]\n",
    "        src = group_lst['input_id_x'][i][:-1]\n",
    "        trg = group_lst['input_id_y'][i][:-1]\n",
    "        while len(src) + len(trg) > seq_len - 3:\n",
    "            if len(src) > len(trg):\n",
    "                src.pop()\n",
    "            elif len(src) < len(trg):\n",
    "                trg.pop()\n",
    "            else:\n",
    "                src.pop()\n",
    "                trg.pop()\n",
    "        src.append(end_token)\n",
    "        trg.append(end_token)\n",
    "\n",
    "        lst.append(src + [tokenizer.sep_token_id] + trg)\n",
    "        mask.append([0] * (len(src) + 1))\n",
    "    group_lst['input_ids'] = lst\n",
    "    group_lst['input_mask'] = mask\n",
    "    return group_lst\n",
    "\n",
    "# Function to pad sequences\n",
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    mask_ = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
    "    return result\n",
    "\n",
    "def pad_function(group_lst):\n",
    "    max_length = seq_len\n",
    "    group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], tokenizer.pad_token_id, max_length)\n",
    "    group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "    return group_lst\n",
    "\n",
    "# Apply merge and mask to the tokenized datasets\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_valid_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    merge_and_mask,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    desc=\"Merging and masking\"\n",
    ")\n",
    "\n",
    "# Apply padding to the datasets\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    pad_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    desc=\"Padding\"\n",
    ")\n",
    "\n",
    "print(\"Merging, masking, and padding complete.\")\n",
    "print(\"Training Set:\", len(lm_datasets['train']))\n",
    "print(\"Validation Set:\", len(lm_datasets['validation']))\n",
    "print(\"Test Set:\", len(lm_datasets['test']))\n",
    "\n",
    "print(lm_datasets, 'padded dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into DatasetDict\n",
    "raw_datasets = datasets.DatasetDict()\n",
    "raw_datasets['train'] = lm_datasets['train']\n",
    "raw_datasets['validation'] = lm_datasets['validation']\n",
    "raw_datasets['test'] = lm_datasets['test']\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, split, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets[split]\n",
    "        self.length = len(self.text_datasets)\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "            input_ids = self.text_datasets[idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets[idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets[idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs\n",
    "\n",
    "# Define model embedding\n",
    "model_emb = lambda x: x  # Placeholder: Replace with actual model embedding function\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = TextDataset(raw_datasets, 'train', model_emb=model_emb)\n",
    "valid_dataset = TextDataset(raw_datasets, 'validation', model_emb=model_emb)\n",
    "test_dataset = TextDataset(raw_datasets, 'test', model_emb=model_emb)\n",
    "\n",
    "# Create data loaders with RandomSampler\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, sampler=RandomSampler(valid_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=RandomSampler(test_dataset))\n",
    "\n",
    "def infinite_data_loader(data_loader, device):\n",
    "    while True:\n",
    "        for batch, cond in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            cond = {k: v.to(device) for k, v in cond.items()}\n",
    "            yield batch, cond\n",
    "            \n",
    "# Convert the data loaders to infinite data loaders           \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = infinite_data_loader(train_loader, device)  # Ensure train_loader returns batches on the correct device\n",
    "valid_loader = infinite_data_loader(valid_loader, device)  # Ensure valid_loader returns batches on the correct device\n",
    "\n",
    "# Sample data from the infinite data loaders\n",
    "train_data_iter = iter(train_loader)\n",
    "valid_data_iter = iter(valid_loader)\n",
    "test_data_iter = iter(test_loader)  # Test data is usually not infinite\n",
    "\n",
    "print(\"Sample from train dataset:\", next(train_data_iter))\n",
    "print(\"Sample from validation dataset:\", next(valid_data_iter))\n",
    "print(\"Sample from test dataset:\", next(test_data_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch as th\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    device = timesteps.device\n",
    "    freqs = th.exp(\n",
    "        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32, device=device) / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME need to define TransformerNetModel\n",
    "#  The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "# Adapted from diffuSeq\n",
    "\n",
    "# TODO code the transformer from scratch\n",
    "# TODO design the transformer from scratch\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dims, hidden_t_dim, output_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        # FIXME set to an actual value\n",
    "        config.hidden_dropout_prob = 0\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        self.output_dims = output_dims\n",
    "        # self.dropout = dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "#         print(\"transformer self.hidden_size\", self.hidden_size)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "        # Generate logits for hidden representation\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "\n",
    "        # Time embeddings\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            # params as input features * output features\n",
    "            nn.Linear(hidden_t_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Function to deal with having a hidden size not equal to input size, project to hidden size (?)\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            # NOTE input_dims = 128\n",
    "            # hidden_size = 768\n",
    "            # self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "            #                                 nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "            # FIXME this is trying to convert to hidden size 768 why??? it's already in the hidden siz \n",
    "            # FIXME this actually doesn't seeem necessary to do????\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(config.hidden_size, input_dims),\n",
    "                                             nn.Tanh(), nn.Linear(input_dims, input_dims))\n",
    "\n",
    "        # FIXME why is this temporary \n",
    "        temp_bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "       \n",
    "       # FIXME why do we do this 2 times????\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "        # self.lm_head.weight.requires_grad = False\n",
    "        # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "        # TODO explain what is happening\n",
    "        self.input_transformers = temp_bert.encoder\n",
    "        # TODO explain what is doing\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "        self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "     \n",
    "        del temp_bert.embeddings\n",
    "        del temp_bert.pooler\n",
    "\n",
    "        # FIXME When does this get used\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # FIXME what is happening here\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "    \n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.word_embedding(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_repr):\n",
    "    # NOTE we make a simplifying assumption get the logits from linear layer\n",
    "        return self.lm_head(hidden_repr)\n",
    "                \n",
    "\n",
    "    # FIXME what is the difference btween BertModel, BertConfig, BertTokenizer, maybe define it all in one place config for tokenizer + embeddings?\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        emb_t = self.time_embed(timestep_embedding(timesteps.to(self.time_embed[0].weight.device), self.hidden_t_dim))  # Ensure timesteps are on the same device\n",
    "        emb_x = x\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, :seq_length].to(x.device)  # Ensure position_ids are on the same device\n",
    "        emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "        input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "        h = input_trans_hidden_states\n",
    "        h = h.type(x.dtype)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c580e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explore other simplistic sample code\n",
    "# https://github.com/lucidrains/denoising-diffusion-pytorch\n",
    "# https://e-dorigatti.github.io/math/deep%20learning/2023/06/25/diffusion.html\n",
    "# https://github.com/tanelp/tiny-diffusion\n",
    "# NOTE adapted from diffuSeq, which is adapted from https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "class GaussianDiffusion():\n",
    "    def __init__(self, betas, predict_xstart=True):\n",
    "        self.predict_xstart = predict_xstart\n",
    "        self.rescale_timesteps = True\n",
    "\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    # NOTE the below comments from diffuSeq\n",
    "    # self.mapping_func = None # implement in train main()\n",
    "    # self.add_mask_noise = False # TODO\n",
    "\n",
    "    # FIXME copied directly from diffuSeq\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "    \n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        device = x_t.device\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape, device) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape, device) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        device = x_t.device\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape, device) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape, device)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "    \n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        device = x_start.device\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape, device) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape, device)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape, device\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "        device = x_start.device\n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape, device) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape, device) * noise\n",
    "        )\n",
    "\n",
    "        mask = torch.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape).to(device)\n",
    "        return torch.where(mask==0, x_start, x_t)\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        device = x_start.device\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape, device) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape, device) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape, device)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape, device\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "        top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = th.randn_like(x)\n",
    "            replace_mask = th.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = th.randn_like(noise[replace_mask])\n",
    "                replace_mask = th.abs(noise) > top_p\n",
    "            assert (th.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = th.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask is None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"],\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance for the diffusion posterior at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'mean': the posterior mean.\n",
    "                 - 'variance': the posterior variance.\n",
    "                 - 'log_variance': the log of the posterior variance.\n",
    "                 - 'pred_xstart': the predicted x_0.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "        model_output = model(x, t, **model_kwargs).to(device)\n",
    "        if denoised_fn is not None:\n",
    "            model_output = denoised_fn(model_output)\n",
    "        pred_xstart = self._predict_xstart_from_eps(x, t, model_output)\n",
    "        if clip_denoised:\n",
    "            pred_xstart = pred_xstart.clamp(-1, 1)\n",
    "\n",
    "        posterior_mean, posterior_variance, posterior_log_variance = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": posterior_mean,\n",
    "            \"variance\": posterior_variance,\n",
    "            \"log_variance\": posterior_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            if clamp_step is not None:\n",
    "                if not clamp_first:\n",
    "                    if i > clamp_step:\n",
    "                        denoised_fn_cur = None\n",
    "                    else:\n",
    "                        denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    if i >= clamp_step:\n",
    "                        denoised_fn_cur = denoised_fn\n",
    "                    else:\n",
    "                        denoised_fn_cur = None\n",
    "            else:\n",
    "                denoised_fn_cur = denoised_fn\n",
    "\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = th.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        return x_start_mean + std * noise\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "\n",
    "        # Ensure input_ids is a LongTensor\n",
    "        input_ids = input_ids.long()\n",
    "\n",
    "        loss_fct = th.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "\n",
    "        if mask is not None:\n",
    "            decoder_nll *= mask\n",
    "\n",
    "        if mask is not None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1) / mask.sum(dim=-1)\n",
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1)\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t):\n",
    "        device = x.device\n",
    "        pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "        pred_prev, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n",
    "        return {'pred_xprev': pred_prev, 'pred_xstart': pred_xstart}\n",
    "\n",
    "    def training_losses_seq2seq(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        device = x_start.device\n",
    "        x_start_fix = x_start.to(device)\n",
    "\n",
    "        input_ids_x = model_kwargs.pop('input_ids').long().to(device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask').to(device)\n",
    "        x_start_mean = model.get_embeds(input_ids_x).to(device)\n",
    "\n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, torch.tensor([0]).to(device), x_start_mean.shape, device)\n",
    "        x_start = self._get_x_start(x_start_mean, std).to(device)\n",
    "\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start).to(device)\n",
    "\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask).to(device)\n",
    "\n",
    "        get_logits = model.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs).to(device)\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2).to(device)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart'].to(device)\n",
    "        t0_mask = (t == 0).to(device)\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2).to(device)\n",
    "        terms[\"mse\"] = terms[\"mse\"].to(device)\n",
    "        terms[\"mse\"] = torch.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, torch.LongTensor([self.num_timesteps - 1]).to(device))\n",
    "        tT_loss = mean_flat(out_mean ** 2).to(device)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x).to(device)\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t).to(device)\n",
    "\n",
    "        terms[\"loss\"] = (terms[\"mse\"] + decoder_nll + tT_loss).to(device)\n",
    "\n",
    "        return terms\n",
    "\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape, device)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape, device)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = th.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n",
    "            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        if langevin_fn:\n",
    "            sample = langevin_fn(sample, mean_pred, sigma, self.alphas_cumprod_prev[t[0]], t, x)\n",
    "        \n",
    "        if mask is None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_reverse_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape, device) * x\n",
    "            - out[\"pred_xstart\"]\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape, device)\n",
    "        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape, device)\n",
    "\n",
    "        # Equation 12. reversed\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n",
    "            + th.sqrt(1 - alpha_bar_next) * eps\n",
    "        )\n",
    "\n",
    "        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape, device):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :param device: the device to move the tensor to.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = torch.from_numpy(arr).to(device=device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38841c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler():\n",
    "    \"\"\"\n",
    "    A distribution over timesteps in the diffusion process, intended to reduce\n",
    "    variance of the objective.\n",
    "\n",
    "    Sampler performs unbiased importance sampling, in which the\n",
    "    objective's mean is unchanged.\n",
    "    TODO confirm & update comment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diffusion):\n",
    "        self.diffusion = diffusion\n",
    "        self._weights = np.ones([diffusion.num_timesteps])\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Importance-sample timesteps for a batch.\n",
    "\n",
    "        :param batch_size: the number of timesteps.\n",
    "        :param device: the torch device to save to.\n",
    "        :return: a tuple (timesteps, weights):\n",
    "                 - timesteps: a tensor of timestep indices.\n",
    "                 - weights: a tensor of weights to scale the resulting losses.\n",
    "        \"\"\"\n",
    "        w = self.weights()\n",
    "        p = w / np.sum(w)\n",
    "        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n",
    "        indices = th.from_numpy(indices_np).long()#.to(device)\n",
    "        weights_np = 1 / (len(p) * p[indices_np])\n",
    "        weights = th.from_numpy(weights_np).float()#.to(device)\n",
    "        return indices, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11284a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training loop\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    \"\"\"\n",
    "    Update target parameters to be closer to those of source parameters using\n",
    "    an exponential moving average.\n",
    "\n",
    "    :param target_params: the target parameter sequence.\n",
    "    :param source_params: the source parameter sequence.\n",
    "    :param rate: the EMA rate (closer to 1 means slower).\n",
    "    \"\"\"\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n",
    "\n",
    "def zero_grad(model_params):\n",
    "    for param in model_params:\n",
    "        # Taken from https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer.add_param_group\n",
    "        if param.grad is not None:\n",
    "            param.grad.detach_()\n",
    "            param.grad.zero_()\n",
    "\n",
    "def log_loss_dict(diffusion, ts, losses):\n",
    "    for key, values in losses.items():\n",
    "        # logger.logkv_mean(key, values.mean().item())\n",
    "        # Log the quantiles (four quartiles, in particular).\n",
    "        for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
    "            quartile = int(4 * sub_t / diffusion.num_timesteps)\n",
    "            # logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW  # Make sure to import AdamW optimizer\n",
    "\n",
    "# Define the noise schedule\n",
    "num_diffusion_timesteps = 2000  # Reduce timesteps for faster experiments\n",
    "scale = 1000 / num_diffusion_timesteps\n",
    "beta_start = scale * 0.0001\n",
    "beta_end = scale * 0.02\n",
    "betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
    "\n",
    "# Instantiate the diffusion model & transformer\n",
    "diffusion = GaussianDiffusion(betas=betas)\n",
    "\n",
    "# Specify correct dimensions\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dims = 128\n",
    "model = TransformerNetModel(vocab_size=vocab_size, input_dims=embedding_dim, hidden_t_dim=hidden_dim, output_dims=output_dims).to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'### The parameter count is {pytorch_total_params}')\n",
    "\n",
    "class TrainLoop():\n",
    "    def __init__(self, model, diffusion, data, batch_size, lr, ema_rate, weight_decay=0.0, learning_steps=0, eval_data=None, eval_interval=-1):\n",
    "        self.model = model.to(device)\n",
    "        self.ddp_model = model\n",
    "        self.diffusion = diffusion\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.microbatch = batch_size\n",
    "        self.lr = lr\n",
    "        self.ema_rate = [ema_rate] if isinstance(ema_rate, float) else [float(x) for x in ema_rate.split(\",\")]\n",
    "        self.schedule_sampler = UniformSampler(diffusion)\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_steps = learning_steps\n",
    "        self.eval_data = eval_data\n",
    "        self.eval_interval = eval_interval\n",
    "        self.step = 0\n",
    "        self.model_params = list(self.model.parameters())\n",
    "        self.master_params = self.model_params\n",
    "        self.opt = AdamW(self.master_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.ema_params = [copy.deepcopy(self.master_params) for _ in range(len(self.ema_rate))]\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def _log_grad_norm(self):\n",
    "        sqsum = 0.0\n",
    "        for p in self.master_params:\n",
    "            if p.grad is not None:\n",
    "                sqsum += (p.grad ** 2).sum().item()\n",
    "\n",
    "    def _anneal_lr(self):\n",
    "        if not self.learning_steps:\n",
    "            return\n",
    "        frac_done = self.step / self.learning_steps\n",
    "        lr = self.lr * (1 - frac_done)\n",
    "        for param_group in self.opt.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def optimize_normal(self):\n",
    "        self._log_grad_norm()\n",
    "        self._anneal_lr()\n",
    "        self.opt.step()\n",
    "        for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "            update_ema(params, self.master_params, rate=rate)\n",
    "\n",
    "    def run_step(self, batch, cond):\n",
    "        batch = batch.to(device)\n",
    "        cond = {k: v.to(device) for k, v in cond.items()}\n",
    "        self.forward_backward(batch, cond)\n",
    "        self.optimize_normal()\n",
    "\n",
    "    def forward_only(self, batch, cond):\n",
    "        with torch.no_grad():\n",
    "            zero_grad(self.model_params)\n",
    "            for i in range(0, batch.shape[0], self.microbatch):\n",
    "                micro = batch[i: i + self.microbatch].to(device)  # Move batch to GPU\n",
    "                micro_cond = {k: v[i: i + self.microbatch].to(device) for k, v in cond.items()}  # Move cond to GPU\n",
    "                t, weights = self.schedule_sampler.sample(micro.shape[0], device)\n",
    "                weights = weights.to(device)  # Ensure weights is on GPU\n",
    "                losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "                log_loss_dict(self.diffusion, t, {f\"eval_{k}\": v * weights for k, v in losses.items()})\n",
    "                self.eval_losses.append(losses['loss'].mean().item())\n",
    "\n",
    "    def forward_backward(self, batch, cond):\n",
    "        zero_grad(self.model_params)\n",
    "        for i in range(0, batch.shape[0], self.microbatch):\n",
    "            micro = batch[i: i + self.microbatch].to(device)  # Move batch to GPU\n",
    "            micro_cond = {k: v[i: i + self.microbatch].to(device) for k, v in cond.items()}  # Move cond to GPU\n",
    "            t, weights = self.schedule_sampler.sample(micro.shape[0], device)\n",
    "            weights = weights.to(device)  # Ensure weights is on GPU\n",
    "            losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "            loss = (losses[\"loss\"] * weights).mean()\n",
    "            log_loss_dict(self.diffusion, t, {k: v * weights for k, v in losses.items()})\n",
    "            loss.backward()\n",
    "            self.train_losses.append(loss.item())\n",
    "\n",
    "    def run_loop(self):\n",
    "        try:\n",
    "            with tqdm(total=self.learning_steps, desc=\"Training Progress\") as pbar:\n",
    "                while not self.learning_steps or self.step < self.learning_steps:\n",
    "                    batch, cond = next(self.data)\n",
    "                    self.run_step(batch, cond)\n",
    "                    if self.eval_data is not None and self.step % self.eval_interval == 0:\n",
    "                        batch_eval, cond_eval = next(self.eval_data)\n",
    "                        self.forward_only(batch_eval, cond_eval)\n",
    "                    self.step += 1\n",
    "                    pbar.update(1)\n",
    "        except StopIteration:\n",
    "            print(\"Data loader exhausted. Saving the model...\")\n",
    "\n",
    "        # Save the model after training is completed\n",
    "        if not os.path.exists('checkpoints'):\n",
    "            os.makedirs('checkpoints')\n",
    "        torch.save(self.model.state_dict(), 'checkpoints/trained_model.pth')\n",
    "        print(\"Model saved successfully.\")\n",
    "        \n",
    "        # Plotting the training and validation losses\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        if self.eval_losses:\n",
    "            plt.plot(self.eval_losses, label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss over Time')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Adjusting the learning steps and validation interval\n",
    "learning_steps = 2000\n",
    "eval_interval = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = infinite_data_loader(train_loader, device)  # Ensure train_loader returns batches on the correct device\n",
    "valid_loader = infinite_data_loader(valid_loader, device)  # Ensure valid_loader returns batches on the correct device\n",
    "\n",
    "TrainLoop(\n",
    "    model=model.to(device),\n",
    "    diffusion=diffusion,\n",
    "    data=iter(train_loader),\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    ema_rate=ema_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_steps=learning_steps,\n",
    "    eval_data=iter(valid_loader),\n",
    "    eval_interval=eval_interval\n",
    ").run_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700de18b",
   "metadata": {},
   "source": [
    "Instantiate all the classes for training loop <br>\n",
    "Set parameters for training\n",
    "\n",
    "Note the implementation details in DiffuSeq (first version) is\n",
    "\"The maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2000\n",
    "and a square-root noise schedule.\"\n",
    "\n",
    "How is it different in v2 or other papers?\n",
    "\n",
    "We have reached StopIteration Exception. Training completed.\n",
    "\n",
    "Get sample output using the forward step of the trained model.\n",
    "\n",
    "## Inference step (sampling / generation part)\n",
    "\n",
    "Once the training completed, we can start the inference step and get cross validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04031ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerNetModel(\n",
    "    vocab_size=vocab_size, \n",
    "    input_dims=embedding_dim, \n",
    "    hidden_t_dim=hidden_dim, \n",
    "    output_dims=output_dims\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('checkpoints/trained_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def temperature_sampling(logits, temperature):\n",
    "    logits = logits / temperature\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=128, num_timesteps=2000, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input prompt\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        # Get the embeddings for the input prompt\n",
    "        input_embeds = model.word_embedding(input_ids).to(device)\n",
    "\n",
    "        # Initialize noise\n",
    "        noise = torch.randn_like(input_embeds).to(device)\n",
    "\n",
    "        # Set up the diffusion process\n",
    "        diffusion = GaussianDiffusion(betas=np.linspace(1e-4, 0.02, num_timesteps))\n",
    "\n",
    "        # Sample from the model using p_sample_loop\n",
    "        samples = diffusion.p_sample_loop(\n",
    "            model=model,\n",
    "            shape=input_embeds.shape,\n",
    "            noise=noise,\n",
    "            device=device,\n",
    "            progress=True,\n",
    "            clamp_step=None  # Set this to a specific value if needed\n",
    "        )\n",
    "\n",
    "        # Convert the generated embeddings back to tokens\n",
    "        generated_ids = model.lm_head(samples[-1].to(device))\n",
    "\n",
    "        # Apply temperature sampling\n",
    "        generated_ids = generated_ids.view(-1, generated_ids.size(-1))  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "        sampled_ids = temperature_sampling(generated_ids, temperature)\n",
    "        sampled_ids = sampled_ids.view(1, -1)  # Reshape back to (1, seq_len)\n",
    "        \n",
    "        generated_text = tokenizer.decode(sampled_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        # Print input IDs and output IDs\n",
    "        print(f\"Input IDs: {input_ids}\")\n",
    "        print(f\"Output IDs: {sampled_ids}\")\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is the capital of Australia?\"\n",
    "generated_response = generate_text(model, tokenizer, prompt, temperature=0.7)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Response: {generated_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47021c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
