{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model for Open Dialogue\n",
    "\n",
    "Simplified version\n",
    "\n",
    "References\n",
    "- DiffuSeq (cited below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "\n",
    "[1] @inproceedings{gong2022diffuseq,\n",
    "  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  booktitle = {International Conference on Learning Representations, ICLR},\n",
    "  title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},\n",
    "  year = 2023\n",
    "}\n",
    "\n",
    "[2] @article{gong2023diffuseqv2,\n",
    "  title={DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models},\n",
    "  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  journal={arXiv preprint arXiv:2310.05793},\n",
    "  year={2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 269,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adapt from other codes\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Use Commonsense Conversation dataset (from Reddit)\n",
    "\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note that, in DiffuSeq, a model file is created to store all training progress, configuration etc. (in bash format poitning to raw files?)\n",
    "\n",
    "- denoise rate ?\n",
    "- using updates in v2 diffuseq took it from 2 days -> 11 hr learning time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiffuSeq [1]\n",
    "12 layers of Transformer with 12 attention\n",
    "heads, where the time step embedding is plugged akin to the position embedding. \n",
    "The\n",
    "maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2;000\n",
    "and a square-root noise schedule. To reduce the out-of-vocabulary generation, we apply Byte Pair\n",
    "Encoding (Sennrich et al., 2016) to construct the vocabulary."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 270,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding dimension = 128\n",
    "embedding_dim = 128\n",
    "\n",
    "# hidden size of time embedding\n",
    "hidden_dim = 128 \n",
    "\n",
    "# :param seq_len: the max sequence length (one-side).\n",
    "seq_len = 128 \n",
    "\n",
    "# TODO good value for this\n",
    "output_dims = 128\n",
    "\n",
    "# Same as diffuSeq\n",
<<<<<<< HEAD
    "num_diffusion_timesteps = 500\n",
=======
    "num_diffusion_timesteps = 2000\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "\n",
    "lr=1e-04\n",
    "\n",
    "# TODO figure out what are the right params to recreate diffuSeq\n",
<<<<<<< HEAD
    "batch_size = 10\n",
    "lr = 0.001 # learning rate\n",
    "ema_rate = 0.999\n",
    "weight_decay = 0.01\n",
    "learning_steps = 500"
=======
    "batch_size = 20 \n",
    "lr = 0.001 # learning rate\n",
    "ema_rate = 0.999\n",
    "weight_decay = 0.01\n",
    "learning_steps = 2000"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": 271,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# use GPU if available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tokenizer from BERT"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 272,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding function $EMB(w)$ to map the discrete text $w$ into a continuous space."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 273,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
<<<<<<< HEAD
       "tensor([[ 0.4484, -0.1596,  0.1920,  ...,  2.5458, -1.5077,  0.1616],\n",
       "        [-0.0226,  0.6659,  0.8042,  ...,  1.3432,  0.6950, -0.7146],\n",
       "        [ 0.8009,  0.6580,  0.1015,  ...,  0.5096, -0.9872,  0.0637],\n",
       "        ...,\n",
       "        [ 0.0669,  0.1094, -0.4716,  ..., -0.0306,  0.9230,  0.2315],\n",
       "        [ 0.5799,  0.8844,  0.5267,  ...,  0.8987, -1.0831, -0.8841],\n",
       "        [-0.7474,  0.2025,  2.8284,  ..., -1.5673, -0.1408, -0.6253]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 36,
=======
       "tensor([[-0.3378,  0.0324, -1.3902,  ...,  0.1729, -2.6803, -0.7593],\n",
       "        [ 0.1732, -1.2004,  0.6624,  ..., -0.8782, -1.7312,  0.2391],\n",
       "        [-2.4312,  2.5413, -1.1820,  ...,  1.0274, -0.5042, -0.9590],\n",
       "        ...,\n",
       "        [-0.2986, -0.3550,  0.9475,  ..., -0.2667,  0.7559, -1.1755],\n",
       "        [ 0.3227,  0.6499,  0.1783,  ...,  0.3180,  1.7025, -2.1005],\n",
       "        [ 0.5641, -0.7157, -0.9514,  ...,  0.5393, -1.5976,  1.0735]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 273,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = torch.nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
    "\n",
    "# initialize random embeddings\n",
    "torch.nn.init.normal_(model_emb.weight)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 274,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 128)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 37,
=======
     "execution_count": 274,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sample text data from file"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Samples: 3382137\n",
      "Validation Data Samples: 2048\n",
      "Test Data Samples: 10000\n"
     ]
    }
   ],
=======
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "source": [
    "# read in the data in training data json file \n",
    "# TODO do this in a different way \n",
    "# TODO load actual dataset from Amazon\n",
    "\n",
    "import json\n",
    "\n",
<<<<<<< HEAD
    "# data_dir = \"./datasets/sample\"\n",
    "# path = f'{data_dir}/train.jsonl'\n",
    "\n",
    "# sentence_lst = {'src':[], 'trg': []}\n",
    "# with open(path, 'r') as f_reader:\n",
    "#         for row in f_reader:\n",
    "#             content = json.loads(row)\n",
    "#             sentence_lst['src'].append(content['src'].strip())\n",
    "#             sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "# TODO use pandas to load faster? any other package can just load json directly rather than row by row\n",
    "\n",
    "data_dir = \"./datasets\"\n",
    "train_path = f'{data_dir}/train_full.jsonl'\n",
    "valid_path = f'{data_dir}/valid_full.jsonl'\n",
    "test_path = f'{data_dir}/test_full.jsonl'\n",
    "\n",
    "def load_data(path, limit=None):\n",
    "    sentence_lst = {'src':[], 'trg': []}\n",
    "    with open(path, 'r') as f_reader:\n",
    "        for i, row in enumerate(f_reader):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "    return sentence_lst\n",
    "\n",
    "# Load datasets with size restriction\n",
    "train_limit = None  # Limit the size of the training set\n",
    "valid_limit = None   # Limit the size of the validation set\n",
    "test_limit = None    # Limit the size of the test set\n",
    "\n",
    "train_data = load_data(train_path, limit=train_limit)\n",
    "valid_data = load_data(valid_path, limit=valid_limit)\n",
    "test_data = load_data(test_path, limit=test_limit)\n",
    "\n",
    "print(\"Training Data Samples:\", len(train_data['src']))\n",
    "print(\"Validation Data Samples:\", len(valid_data['src']))\n",
    "print(\"Test Data Samples:\", len(test_data['src']))\n"
=======
    "data_dir = \"./datasets/sample\"\n",
    "path = f'{data_dir}/train.jsonl'\n",
    "\n",
    "sentence_lst = {'src':[], 'trg': []}\n",
    "with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "# TODO use pandas to load faster? any other package can just load json directly rather than row by row"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 276,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "'jesus , what kind of concerts do you go to where people sucker punch you for being born tall ?'"
      ]
     },
     "execution_count": 39,
=======
       "\"this is my favorite story arc . ca n't wait to see how he does in the tourney ! the show is my guarantee smile for the week .\""
      ]
     },
     "execution_count": 276,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "train_data['src'][0]"
=======
    "sentence_lst['src'][0]"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": 277,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src', 'trg'],\n",
<<<<<<< HEAD
       "    num_rows: 3382137\n",
       "})"
      ]
     },
     "execution_count": 40,
=======
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 277,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
<<<<<<< HEAD
    "raw_datasets = Dataset.from_dict(train_data)\n",
=======
    "raw_datasets = Dataset.from_dict(sentence_lst)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 278,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "{'src': 'jesus , what kind of concerts do you go to where people sucker punch you for being born tall ?',\n",
       " 'trg': 'the kind that allow bitter short people in . so basically all of them .'}"
      ]
     },
     "execution_count": 41,
=======
       "{'src': \"this is my favorite story arc . ca n't wait to see how he does in the tourney ! the show is my guarantee smile for the week .\",\n",
       " 'trg': \"yea it 's hard not to have a smile on your face the entire episode\"}"
      ]
     },
     "execution_count": 278,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing training dataset (num_proc=4): 100%|█████████████████████| 3382137/3382137 [09:18<00:00, 6060.11 examples/s]\n",
      "Tokenizing validation dataset (num_proc=4): 100%|██████████████████████████| 2048/2048 [00:04<00:00, 460.43 examples/s]\n",
      "Tokenizing test dataset (num_proc=4): 100%|█████████████████████████████| 10000/10000 [00:05<00:00, 1937.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Training Set: 3382137\n",
      "Validation Set: 2048\n",
      "Test Set: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
=======
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=4): 100%|████████████████████████████████| 19/19 [00:03<00:00,  4.88 examples/s]\n"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Initialize tokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from functools import partial\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
=======
    "from functools import partial\n",
    "vocab_dict = tokenizer.get_vocab()\n",
    "\n",
    "def tokenize_function(examples,tokenizer):\n",
    "    \n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "    input_id_x = tokenizer(examples['src'], add_special_tokens=True)['input_ids']\n",
    "    input_id_y = tokenizer(examples['trg'], add_special_tokens=True)['input_ids']\n",
    "    result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "    return result_dict\n",
    "\n",
    "# Use partial to pass the tokenizer to the tokenize_function\n",
    "tokenize_function_with_tokenizer = partial(tokenize_function, tokenizer=tokenizer)\n",
    "\n",
<<<<<<< HEAD
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "valid_dataset = Dataset.from_dict(valid_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
=======
    "tokenized_datasets = raw_datasets.map(\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "    tokenize_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
<<<<<<< HEAD
    "    desc=\"Tokenizing training dataset\",\n",
    ")\n",
    "\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    tokenize_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing validation dataset\",\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function_with_tokenizer,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=['src', 'trg'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing test dataset\",\n",
    ")\n",
    "\n",
    "# Combine into DatasetDict\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_valid_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n",
    "print(\"Tokenization complete.\")\n",
    "print(\"Training Set:\", len(tokenized_datasets['train']))\n",
    "print(\"Validation Set:\", len(tokenized_datasets['validation']))\n",
    "print(\"Test Set:\", len(tokenized_datasets['test']))\n",
    "\n",
    "# tokenized_datasets\n",
    "# len(tokenized_datasets['train'][\"input_id_x\"][0])"
=======
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_id_x', 'input_id_y'],\n",
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets[\"input_id_x\"][0])"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging and masking: 100%|█████████████████████████████████████████| 3382137/3382137 [01:46<00:00, 31724.29 examples/s]\n",
      "Merging and masking: 100%|███████████████████████████████████████████████| 2048/2048 [00:00<00:00, 30111.07 examples/s]\n",
      "Merging and masking: 100%|█████████████████████████████████████████████| 10000/10000 [00:00<00:00, 33325.98 examples/s]\n",
      "Padding: 100%|█████████████████████████████████████████████████████| 3382137/3382137 [03:53<00:00, 14514.07 examples/s]\n",
      "Padding: 100%|███████████████████████████████████████████████████████████| 2048/2048 [00:00<00:00, 13470.67 examples/s]\n",
      "Padding: 100%|█████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 13866.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging, masking, and padding complete.\n",
      "Training Set: 3382137\n",
      "Validation Set: 2048\n",
      "Test Set: 10000\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "        num_rows: 3382137\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "        num_rows: 2048\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "}) padded dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to merge and mask sequences\n",
    "def merge_and_mask(group_lst):\n",
    "    lst = []\n",
    "    mask = []\n",
    "    for i in range(len(group_lst['input_id_x'])):\n",
    "        end_token = group_lst['input_id_x'][i][-1]\n",
    "        src = group_lst['input_id_x'][i][:-1]\n",
    "        trg = group_lst['input_id_y'][i][:-1]\n",
    "        while len(src) + len(trg) > seq_len - 3:\n",
    "            if len(src) > len(trg):\n",
    "                src.pop()\n",
    "            elif len(src) < len(trg):\n",
    "                trg.pop()\n",
    "            else:\n",
    "                src.pop()\n",
    "                trg.pop()\n",
    "        src.append(end_token)\n",
    "        trg.append(end_token)\n",
    "\n",
    "        lst.append(src + [tokenizer.sep_token_id] + trg)\n",
    "        mask.append([0] * (len(src) + 1))\n",
    "    group_lst['input_ids'] = lst\n",
    "    group_lst['input_mask'] = mask\n",
    "    return group_lst\n",
    "\n",
    "# Function to pad sequences\n",
=======
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to collate the batch\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    mask_ = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
<<<<<<< HEAD
    "    return result\n",
    "\n",
=======
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "merge and mask: 100%|█████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 4748.65 examples/s]\n",
      "padding: 100%|████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 3799.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def merge_and_mask(group_lst):\n",
    "        lst = []\n",
    "        mask = []\n",
    "        for i in range(len(group_lst['input_id_x'])):\n",
    "            end_token = group_lst['input_id_x'][i][-1]\n",
    "            src = group_lst['input_id_x'][i][:-1]\n",
    "            trg = group_lst['input_id_y'][i][:-1]\n",
    "            while len(src) + len(trg) > seq_len - 3:\n",
    "                if len(src)>len(trg):\n",
    "                    src.pop()\n",
    "                elif len(src)<len(trg):\n",
    "                    trg.pop()\n",
    "                else:\n",
    "                    src.pop()\n",
    "                    trg.pop()\n",
    "            src.append(end_token)\n",
    "            trg.append(end_token)\n",
    "\n",
    "            lst.append(src + [tokenizer.sep_token_id] + trg)\n",
    "            mask.append([0]*(len(src)+1))\n",
    "        group_lst['input_ids'] = lst\n",
    "        group_lst['input_mask'] = mask\n",
    "        return group_lst\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "        merge_and_mask,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"merge and mask\",\n",
    "    )\n",
    "    \n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "def pad_function(group_lst):\n",
    "    max_length = seq_len\n",
    "    group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], tokenizer.pad_token_id, max_length)\n",
    "    group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "    return group_lst\n",
    "\n",
<<<<<<< HEAD
    "# Apply merge and mask to the tokenized datasets\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_valid_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    merge_and_mask,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    desc=\"Merging and masking\"\n",
    ")\n",
    "\n",
    "# Apply padding to the datasets\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    pad_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    desc=\"Padding\"\n",
    ")\n",
    "\n",
    "print(\"Merging, masking, and padding complete.\")\n",
    "print(\"Training Set:\", len(lm_datasets['train']))\n",
    "print(\"Validation Set:\", len(lm_datasets['validation']))\n",
    "print(\"Test Set:\", len(lm_datasets['test']))\n",
    "\n",
    "print(lm_datasets, 'padded dataset')"
=======
    "lm_datasets = tokenized_datasets.map(\n",
    "        pad_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"padding\",\n",
    "    )\n"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 284,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Sample from train dataset: [tensor([[  101.,  4441.,  1010.,  ...,     0.,     0.,     0.],\n",
      "        [  101.,  2471.,  2035.,  ...,     0.,     0.,     0.],\n",
      "        [  101.,  2762.,  1029.,  ...,     0.,     0.,     0.],\n",
      "        ...,\n",
      "        [  101.,  2129.,  2079.,  ...,     0.,     0.,     0.],\n",
      "        [  101.,  3398.,  1010.,  ...,     0.,     0.,     0.],\n",
      "        [  101.,  1016., 11503.,  ...,     0.,     0.,     0.]]), {'input_ids': tensor([[  101,  4441,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  2471,  2035,  ...,     0,     0,     0],\n",
      "        [  101,  2762,  1029,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2129,  2079,  ...,     0,     0,     0],\n",
      "        [  101,  3398,  1010,  ...,     0,     0,     0],\n",
      "        [  101,  1016, 11503,  ...,     0,     0,     0]], dtype=torch.int32), 'input_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], dtype=torch.int32)}]\n",
      "Sample from validation dataset: [tensor([[ 101., 2023., 2003.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 2009., 3957.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 2130., 2065.,  ...,    0.,    0.,    0.],\n",
      "        ...,\n",
      "        [ 101., 2021., 2129.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 2241., 2006.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 1045., 2228.,  ...,    0.,    0.,    0.]]), {'input_ids': tensor([[ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2009, 3957,  ...,    0,    0,    0],\n",
      "        [ 101, 2130, 2065,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2021, 2129,  ...,    0,    0,    0],\n",
      "        [ 101, 2241, 2006,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2228,  ...,    0,    0,    0]], dtype=torch.int32), 'input_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], dtype=torch.int32)}]\n",
      "Sample from test dataset: [tensor([[ 101., 2023., 8239.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 1044., 2232.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 2651., 1045.,  ...,    0.,    0.,    0.],\n",
      "        ...,\n",
      "        [ 101., 2074., 2019.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 2168., 2518.,  ...,    0.,    0.,    0.],\n",
      "        [ 101., 2108., 2013.,  ...,    0.,    0.,    0.]]), {'input_ids': tensor([[ 101, 2023, 8239,  ...,    0,    0,    0],\n",
      "        [ 101, 1044, 2232,  ...,    0,    0,    0],\n",
      "        [ 101, 2651, 1045,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2074, 2019,  ...,    0,    0,    0],\n",
      "        [ 101, 2168, 2518,  ...,    0,    0,    0],\n",
      "        [ 101, 2108, 2013,  ...,    0,    0,    0]], dtype=torch.int32), 'input_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], dtype=torch.int32)}]\n"
=======
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 19\n",
      "}) padded dataset\n"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch as th\n",
    "\n",
    "# Combine into DatasetDict\n",
    "raw_datasets = datasets.DatasetDict()\n",
    "raw_datasets['train'] = lm_datasets['train']\n",
    "raw_datasets['validation'] = lm_datasets['validation']\n",
    "raw_datasets['test'] = lm_datasets['test']\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, split, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets[split]\n",
    "        self.length = len(self.text_datasets)\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "            input_ids = self.text_datasets[idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets[idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets[idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs\n",
    "\n",
    "# Define model embedding\n",
    "model_emb = lambda x: x  # Placeholder: Replace with actual model embedding function\n",
    "\n",
    "# Create datasets for training, validation, and test sets\n",
    "train_dataset = TextDataset(raw_datasets, 'train', model_emb=model_emb)\n",
    "valid_dataset = TextDataset(raw_datasets, 'validation', model_emb=model_emb)\n",
    "test_dataset = TextDataset(raw_datasets, 'test', model_emb=model_emb)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Sample data from the datasets\n",
    "train_data_iter = iter(train_loader)\n",
    "valid_data_iter = iter(valid_loader)\n",
    "test_data_iter = iter(test_loader)\n",
    "\n",
    "print(\"Sample from train dataset:\", next(train_data_iter))\n",
    "print(\"Sample from validation dataset:\", next(valid_data_iter))\n",
    "print(\"Sample from test dataset:\", next(test_data_iter))\n"
=======
    "print(lm_datasets, 'padded dataset')"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# raw_datasets = datasets.DatasetDict()\n",
    "# raw_datasets['train'] = lm_datasets['train']\n",
    "# # raw_datasets\n",
    "# # raw_datasets['train']"
=======
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "raw_datasets = datasets.DatasetDict()\n",
    "raw_datasets['train'] = lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
       "    num_rows: 19\n",
       "})"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into iterable data variable"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# import torch as th\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, text_datasets, model_emb=None):\n",
    "#         super().__init__()\n",
    "#         self.text_datasets = text_datasets\n",
    "#         self.length = len(self.text_datasets['train'])\n",
    "#         self.model_emb = model_emb\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "#             hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "#             # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "#             arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "#             out_kwargs = {}\n",
    "#             out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "#             out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "\n",
    "#             return arr, out_kwargs"
=======
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch as th\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets\n",
    "        self.length = len(self.text_datasets['train'])\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE we need to be able to decode the tokens back to text space, function is available for that"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# train_dataset = TextDataset(raw_datasets, model_emb=model_emb)\n",
    "\n",
    "# data_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=batch_size,\n",
    "# )\n",
    "\n",
    "# data = iter(data_loader)\n",
    "\n",
    "# # NOTE don't use next here since we only have 1 batch to use\n",
    "# # next(data)\n",
    "# # NOTE load data for the validation, inference differently"
=======
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "train_dataset = TextDataset(raw_datasets, model_emb=model_emb)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "data = iter(data_loader)\n",
    "\n",
    "# NOTE don't use next here since we only have 1 batch to use\n",
    "# next(data)\n",
    "# NOTE load data for the validation, inference differently"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE next() will get the next batch"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.dataset[0]"
=======
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.325847  ,  0.65916157, -0.269264  , ..., -0.07905941,\n",
       "         -0.52941495, -1.9351798 ],\n",
       "        [-0.21605405, -0.85761446,  1.7645719 , ..., -1.5625378 ,\n",
       "         -0.8019739 , -0.41121206],\n",
       "        [-0.4371445 ,  0.6369445 ,  0.6457014 , ..., -0.5155643 ,\n",
       "         -0.426604  , -0.12025625],\n",
       "        ...,\n",
       "        [-0.337817  ,  0.03241294, -1.3901956 , ...,  0.17290582,\n",
       "         -2.680321  , -0.7593272 ],\n",
       "        [-0.337817  ,  0.03241294, -1.3901956 , ...,  0.17290582,\n",
       "         -2.680321  , -0.7593272 ],\n",
       "        [-0.337817  ,  0.03241294, -1.3901956 , ...,  0.17290582,\n",
       "         -2.680321  , -0.7593272 ]], dtype=float32),\n",
       " {'input_ids': array([  101,  2023,  2003,  2026,  5440,  2466,  8115,  1012,  6187,\n",
       "          1050,  1005,  1056,  3524,  2000,  2156,  2129,  2002,  2515,\n",
       "          1999,  1996,  2778,  5420,   999,  1996,  2265,  2003,  2026,\n",
       "         11302,  2868,  2005,  1996,  2733,  1012,   102,   102,   101,\n",
       "          6300,  2050,  2009,  1005,  1055,  2524,  2025,  2000,  2031,\n",
       "          1037,  2868,  2006,  2115,  2227,  1996,  2972,  2792,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       "  'input_mask': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])})"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[0]"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and diffusion"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
=======
   "execution_count": 291,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model = TransformerNetModel(\n",
    "#         input_dims=hidden_dim,\n",
    "#         output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "#         hidden_t_dim=hidden_t_dim,\n",
    "#         dropout=dropout,\n",
    "#         config_name=config_name,\n",
    "#         vocab_size=vocab_size,\n",
    "#         init_pretrained=use_plm_init\n",
    "#     )\n",
    "\n",
    "#     betas = gd.get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "\n",
    "#     if not timestep_respacing:\n",
    "#         timestep_respacing = [diffusion_steps]\n",
    "\n",
    "#     diffusion = SpacedDiffusion(\n",
    "#         use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "#         betas=betas,\n",
    "#         rescale_timesteps=rescale_timesteps,\n",
    "#         predict_xstart=predict_xstart,\n",
    "#         learn_sigmas = learn_sigma,\n",
    "#         sigma_small = sigma_small,\n",
    "#         use_kl = use_kl,\n",
    "#         rescale_learned_sigmas=rescale_learned_sigmas\n",
    "#     )\n",
    "\n",
    "# FIXME need to implement a way to save the model progress so that\n",
    "# trained model can be loaded later for assessment purposes"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
=======
   "execution_count": 292,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch as th\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    device = timesteps.device\n",
    "    freqs = th.exp(\n",
    "        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32, device=device) / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
=======
   "execution_count": 293,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME need to define TransformerNetModel\n",
    "#  The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "# Adapted from diffuSeq\n",
    "\n",
    "# TODO code the transformer from scratch\n",
    "# TODO design the transformer from scratch\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dims, hidden_t_dim, output_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        # FIXME set to an actual value\n",
    "        config.hidden_dropout_prob = 0\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        self.output_dims = output_dims\n",
    "        # self.dropout = dropout\n",
    "        self.hidden_size = config.hidden_size\n",
<<<<<<< HEAD
    "#         print(\"transformer self.hidden_size\", self.hidden_size)\n",
=======
    "        print(\"transformer self.hidden_size\", self.hidden_size)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "        # Generate logits for hidden representation\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "\n",
    "        # Time embeddings\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            # params as input features * output features\n",
    "            nn.Linear(hidden_t_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Function to deal with having a hidden size not equal to input size, project to hidden size (?)\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            # NOTE input_dims = 128\n",
    "            # hidden_size = 768\n",
    "            # self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "            #                                 nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "            # FIXME this is trying to convert to hidden size 768 why??? it's already in the hidden siz \n",
    "            # FIXME this actually doesn't seeem necessary to do????\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(config.hidden_size, input_dims),\n",
    "                                             nn.Tanh(), nn.Linear(input_dims, input_dims))\n",
    "\n",
    "        # FIXME why is this temporary \n",
    "        temp_bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "       \n",
    "       # FIXME why do we do this 2 times????\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "        # self.lm_head.weight.requires_grad = False\n",
    "        # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "        # TODO explain what is happening\n",
    "        self.input_transformers = temp_bert.encoder\n",
    "        # TODO explain what is doing\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "        self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "     \n",
    "        del temp_bert.embeddings\n",
    "        del temp_bert.pooler\n",
    "\n",
    "        # FIXME When does this get used\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # FIXME what is happening here\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "    \n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.word_embedding(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_repr):\n",
    "    # NOTE we make a simplifying assumption get the logits from linear layer\n",
    "        return self.lm_head(hidden_repr)\n",
    "                \n",
    "\n",
    "    # FIXME what is the difference btween BertModel, BertConfig, BertTokenizer, maybe define it all in one place config for tokenizer + embeddings?\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        # FIXME update string comment\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
<<<<<<< HEAD
    "#         print(\"Forward step\")\n",
=======
    "        print(\"Forward step\")\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        \n",
    "        # print(\"timesteps: a 1-D batch of timesteps:\",timesteps)\n",
    "        # print(\"self.hidden_t_dim\", self.hidden_t_dim)\n",
    "\n",
    "        # Embedded timestep\n",
    "\n",
    "        # print(\"1D batch of timesteps\",timesteps,timesteps.size())\n",
    "        \n",
    "        # Note timestep_embedding returns an N * dim Tensor of positional embeddings\n",
    "        # Note it gives N*128 embeddings\n",
    "\n",
    "        # expects a 1D tensor as input, with the size of the input tensor being (hidden_t_dim,)\n",
    "        emb_t = self.time_embed(timestep_embedding(timesteps, self.hidden_t_dim))\n",
    "\n",
<<<<<<< HEAD
    "#         print(\"x: an [N x C x ...] Tensor of inputs\",\"size:\",x.size())\n",
=======
    "        print(\"x: an [N x C x ...] Tensor of inputs\",\"size:\",x.size())\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        # NOTE x is already of size 19x128x768, so don't need to expand\n",
    "        # FIXME why doesn't this work as expected?\n",
    "        # if self.input_dims != self.hidden_size:\n",
    "        #     # FIXME change this to convert to 19x128x768 to ...x128\n",
    "        #     emb_x = self.input_up_proj(x)\n",
    "        # else:\n",
    "        # FIXME do we want this to get pushed to the correct size?\n",
    "        emb_x = x\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, : seq_length ]\n",
<<<<<<< HEAD
    "#         print(\"emb_x.shape, emb_t.shape, self.position_embeddings\")\n",
    "#         print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
=======
    "        print(\"emb_x.shape, emb_t.shape, self.position_embeddings\")\n",
    "        print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "\n",
    "        input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "\n",
<<<<<<< HEAD
    "#         print(\"input_trans_hideden_states.shape\", input_trans_hidden_states.shape)\n",
=======
    "        print(\"input_trans_hideden_states.shape\", input_trans_hidden_states.shape)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        \n",
    "        # if self.output_dims != self.hidden_size:\n",
    "        #     # FIXME this should allow the output to be projected down to 19x128x128 so that it's osame as input data\n",
    "        #     h = self.output_down_proj(input_trans_hidden_states)\n",
    "        #     print(\"transformed h.shape\", h.shape)\n",
    "        # else:\n",
    "        # FIXME why this transofmration not required, it seems the model output compared for mse calculation is also having end dimension x768\n",
    "        h = input_trans_hidden_states\n",
    "        h = h.type(x.dtype)\n",
    "\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 52,
=======
   "execution_count": 294,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explore other simplistic sample code\n",
    "# https://github.com/lucidrains/denoising-diffusion-pytorch\n",
    "# https://e-dorigatti.github.io/math/deep%20learning/2023/06/25/diffusion.html\n",
    "# https://github.com/tanelp/tiny-diffusion"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 295,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE adapted from diffuSeq, which is adapted from https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "class GaussianDiffusion():\n",
    "    def __init__(self, betas, predict_xstart=True):\n",
    "        self.predict_xstart = predict_xstart\n",
    "        self.rescale_timesteps = True\n",
    "\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    # NOTE the below comments from diffuSeq\n",
    "    # self.mapping_func = None # implement in train main()\n",
    "    # self.add_mask_noise = False # TODO\n",
    "\n",
    "    # FIXME copied directly from diffuSeq\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "    \n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "    \n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "        # FIXME why this isn't working\n",
    "        # if mask == None:\n",
    "        #     return x_t\n",
    "        # else:\n",
    "        mask = th.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape)\n",
    "        return th.where(mask==0, x_start, x_t)\n",
    "        \n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None):\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.size(0), x.size(-1)\n",
    "        assert t.shape == (B,)\n",
    "        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
    "\n",
    "        model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n",
    "        model_log_variance = np.log(np.append(self.posterior_variance[1], self.betas[1:]))\n",
    "\n",
    "        model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                x = denoised_fn(x, t)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "            )\n",
    "\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "            top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = th.randn_like(x)\n",
    "            replace_mask = th.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = th.randn_like(noise[replace_mask])\n",
    "                replace_mask = th.abs(noise) > top_p\n",
    "            assert (th.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = th.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"],\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    \n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            if clamp_step is not None:\n",
    "                if not clamp_first:\n",
    "                    if i > clamp_step:\n",
    "                        denoised_fn_cur = None\n",
    "                    else:\n",
    "                        denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    if i >= clamp_step:\n",
    "                        denoised_fn_cur = denoised_fn\n",
    "                    else:\n",
    "                        denoised_fn_cur = None\n",
    "            else:\n",
    "                denoised_fn_cur = denoised_fn\n",
    "\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = th.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        # print(x_start_mean.device, noise.device)\n",
    "        return (\n",
    "             x_start_mean + std * noise\n",
    "        )\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
<<<<<<< HEAD
    "\n",
    "        # Ensure input_ids is a LongTensor\n",
    "        input_ids = input_ids.long()\n",
    "\n",
    "        loss_fct = th.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "\n",
    "        if mask is not None:\n",
    "            decoder_nll *= mask\n",
    "\n",
    "        if mask is not None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1) / mask.sum(dim=-1)\n",
=======
    "        # print(logits.shape)\n",
    "        loss_fct = th.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll *= mask\n",
    "        # print(decoder_nll.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1)/mask.sum(dim=-1)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1)\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t):\n",
    "\n",
    "        # if self.predict_xstart:\n",
    "        #     pred_xstart = model_output\n",
    "        #     pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "        #         x_start=pred_xstart, x_t=x, t=t\n",
    "        #     )\n",
    "\n",
    "        # else: # predict eps\n",
    "        pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "    \n",
    "        pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        return {'pred_xprev':pred_prev, 'pred_xstart':pred_xstart}\n",
    "\n",
    "    def training_losses_seq2seq(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs. # not used unless fixing the input embeddings\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        x_start_fix = x_start # save the orignal x_0\n",
    "\n",
<<<<<<< HEAD
    "        assert 'input_ids' in model_kwargs\n",
    "        input_ids_x = model_kwargs.pop('input_ids').long().to(t.device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask').to(t.device)\n",
    "        x_start_mean = model.get_embeds(input_ids_x)\n",
    "\n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   th.tensor([0]).to(x_start_mean.device),\n",
    "                                   x_start_mean.shape)\n",
    "        x_start = self._get_x_start(x_start_mean, std)\n",
    "\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start).to(t.device)\n",
    "\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask)\n",
=======
    "        # Note this is size 19 x 128 x 128 \n",
    "        print(\"x_start size\", x_start.size())\n",
    "\n",
    "        assert 'input_ids' in model_kwargs\n",
    "        input_ids_x = model_kwargs.pop('input_ids')#.to(t.device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask')#.to(t.device)\n",
    "        x_start_mean = model.get_embeds(input_ids_x)\n",
    "\n",
    "        # 12 x 128 x 768 once you get the embeds \n",
    "        print(\"x_start_mean size\", x_start_mean.size())\n",
    "        \n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   th.tensor([0]).to(x_start_mean.device),\n",
    "                                   x_start_mean.shape)\n",
    "        # print(std.shape, )\n",
    "        # x_start_log_var = 2 * th.log(std)\n",
    "        x_start = self._get_x_start(x_start_mean, std)\n",
    "        # print(x_start_mean.shape, x_start.shape)\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "\n",
    "        # Diffuse the data in x_start (return a noisy version)\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask) # reparametrization trick.\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "\n",
    "        get_logits = model.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
<<<<<<< HEAD
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart']\n",
=======
    "        # FIXME try to fix dimensionality error\n",
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "        # FIXME why these aren't the same shape, why is this asserted when the conversion shows moving to hidden dimension\n",
    "        # assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart'] # predicted_xstart = model_output\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        t0_mask = (t == 0)\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2)\n",
    "        terms[\"mse\"] = th.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "\n",
<<<<<<< HEAD
    "        out_mean, _, _ = self.q_mean_variance(x_start, th.LongTensor([self.num_timesteps - 1]).to(x_start.device))\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x)\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t)\n",
=======
    "        # tT_mask = (t == self.num_timesteps - 1)\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, th.LongTensor([self.num_timesteps - 1]).to(x_start.device))\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x) # embedding regularization\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t) # x_0->model_out_x_start\n",
    "        # assert (model.lm_head.weight == model.word_embedding.weight).all()\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "\n",
    "        terms[\"loss\"] = terms[\"mse\"] + decoder_nll + tT_loss\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = th.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n",
    "            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # print(sigma.mean())\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        if langevin_fn:\n",
    "            print(t.shape)\n",
    "            sample=langevin_fn(sample, mean_pred, sigma, self.alphas_cumprod_prev[t[0]], t, x)\n",
    "        \n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_reverse_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n",
    "            - out[\"pred_xstart\"]\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n",
    "\n",
    "        # Equation 12. reversed\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n",
    "            + th.sqrt(1 - alpha_bar_next) * eps\n",
    "        )\n",
    "\n",
    "        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
=======
   "execution_count": 296,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler():\n",
    "    \"\"\"\n",
    "    A distribution over timesteps in the diffusion process, intended to reduce\n",
    "    variance of the objective.\n",
    "\n",
    "    Sampler performs unbiased importance sampling, in which the\n",
    "    objective's mean is unchanged.\n",
    "    TODO confirm & update comment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diffusion):\n",
    "        self.diffusion = diffusion\n",
    "        self._weights = np.ones([diffusion.num_timesteps])\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Importance-sample timesteps for a batch.\n",
    "\n",
    "        :param batch_size: the number of timesteps.\n",
    "        :param device: the torch device to save to.\n",
    "        :return: a tuple (timesteps, weights):\n",
    "                 - timesteps: a tensor of timestep indices.\n",
    "                 - weights: a tensor of weights to scale the resulting losses.\n",
    "        \"\"\"\n",
    "        w = self.weights()\n",
    "        p = w / np.sum(w)\n",
    "        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n",
    "        indices = th.from_numpy(indices_np).long()#.to(device)\n",
    "        weights_np = 1 / (len(p) * p[indices_np])\n",
    "        weights = th.from_numpy(weights_np).float()#.to(device)\n",
    "        return indices, weights\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
=======
   "execution_count": 297,
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training loop\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    \"\"\"\n",
    "    Update target parameters to be closer to those of source parameters using\n",
    "    an exponential moving average.\n",
    "\n",
    "    :param target_params: the target parameter sequence.\n",
    "    :param source_params: the source parameter sequence.\n",
    "    :param rate: the EMA rate (closer to 1 means slower).\n",
    "    \"\"\"\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n",
    "\n",
    "def zero_grad(model_params):\n",
    "    for param in model_params:\n",
    "        # Taken from https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer.add_param_group\n",
    "        if param.grad is not None:\n",
    "            param.grad.detach_()\n",
    "            param.grad.zero_()\n",
    "\n",
    "def log_loss_dict(diffusion, ts, losses):\n",
    "    for key, values in losses.items():\n",
    "        # logger.logkv_mean(key, values.mean().item())\n",
    "        # Log the quantiles (four quartiles, in particular).\n",
    "        for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
    "            quartile = int(4 * sub_t / diffusion.num_timesteps)\n",
    "            # logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The parameter count is 110184634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|████▊                                                       | 64/800 [33:37<6:26:40, 31.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader exhausted. Saving the model...\n",
      "Model saved successfully.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW  # Make sure to import AdamW optimizer\n",
    "\n",
    "# Define the noise schedule\n",
    "num_diffusion_timesteps = 1000  # Reduce timesteps for faster experiments\n",
    "scale = 1000 / num_diffusion_timesteps\n",
    "beta_start = scale * 0.0001\n",
    "beta_end = scale * 0.02\n",
    "betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
    "\n",
    "# Instantiate the diffusion model & transformer\n",
    "diffusion = GaussianDiffusion(betas=betas)\n",
    "\n",
    "# Specify correct dimensions\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "output_dims = 128\n",
    "model = TransformerNetModel(vocab_size=vocab_size, input_dims=embedding_dim, hidden_t_dim=hidden_dim, output_dims=output_dims)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'### The parameter count is {pytorch_total_params}')\n",
    "\n",
    "class TrainLoop():\n",
    "    def __init__(self, model, diffusion, data, batch_size, lr, ema_rate, weight_decay=0.0, learning_steps=0, eval_data=None, eval_interval=-1):\n",
    "        self.model = model\n",
    "        self.ddp_model = model\n",
    "        self.diffusion = diffusion\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.microbatch = batch_size\n",
    "        self.lr = lr\n",
    "        self.ema_rate = [ema_rate] if isinstance(ema_rate, float) else [float(x) for x in ema_rate.split(\",\")]\n",
    "        self.schedule_sampler = UniformSampler(diffusion)\n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_steps = learning_steps\n",
    "        self.eval_data = eval_data\n",
    "        self.eval_interval = eval_interval\n",
    "        self.step = 0\n",
    "        self.model_params = list(self.model.parameters())\n",
    "        self.master_params = self.model_params\n",
    "        self.opt = AdamW(self.master_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        self.ema_params = [copy.deepcopy(self.master_params) for _ in range(len(self.ema_rate))]\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
=======
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import copy\n",
    "\n",
    "class TrainLoop():\n",
    "    def __init__(\n",
    "        self,\n",
    "    #     *,\n",
    "        model,\n",
    "        diffusion,\n",
    "        data,\n",
    "        batch_size,\n",
    "    #     microbatch,\n",
    "        lr,\n",
    "        ema_rate,\n",
    "    #     log_interval,\n",
    "        #  schedule_sampler=None,\n",
    "        weight_decay=0.0,\n",
    "        learning_steps=0,\n",
    "    #     checkpoint_path='',\n",
    "    #     gradient_clipping=-1.,\n",
    "        eval_data=None,\n",
    "        eval_interval=-1,\n",
    "    ):\n",
    "          self.model = model\n",
    "          self.ddp_model = model # NOTE no distribution training\n",
    "          self.diffusion = diffusion\n",
    "          self.data = data\n",
    "          self.batch_size = batch_size\n",
    "        #   self.microbatch = microbatch if microbatch > 0 else batch_size\n",
    "          # Assume no microbatch\n",
    "          self.microbatch = batch_size\n",
    "\n",
    "          self.lr = lr\n",
    "          self.ema_rate = (\n",
    "            [ema_rate]\n",
    "            if isinstance(ema_rate, float)\n",
    "            else [float(x) for x in ema_rate.split(\",\")]\n",
    "            )\n",
    "          \n",
    "          # NOTE assuming uniform sampler\n",
    "          self.schedule_sampler = UniformSampler(diffusion)\n",
    "          self.weight_decay = weight_decay\n",
    "          self.learning_steps = learning_steps\n",
    "          self.eval_data = eval_data\n",
    "          self.eval_interval = eval_interval\n",
    "          \n",
    "          self.step = 0\n",
    "\n",
    "          # TODO check other initialization steps are covered\n",
    "\n",
    "          self.model_params = list(self.model.parameters())\n",
    "          self.master_params = self.model_params\n",
    "\n",
    "          # Optimizer\n",
    "          self.opt = AdamW(self.master_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "          self.ema_params = [\n",
    "                copy.deepcopy(self.master_params) for _ in range(len(self.ema_rate))\n",
    "            ]\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "\n",
    "    def _log_grad_norm(self):\n",
    "        sqsum = 0.0\n",
    "        for p in self.master_params:\n",
<<<<<<< HEAD
    "            if p.grad is not None:\n",
    "                sqsum += (p.grad ** 2).sum().item()\n",
=======
    "            if p.grad != None:\n",
    "                sqsum += (p.grad ** 2).sum().item()\n",
    "        # TODO implement logging\n",
    "        # logger.logkv_mean(\"grad_norm\", np.sqrt(sqsum))\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "\n",
    "    def _anneal_lr(self):\n",
    "        if not self.learning_steps:\n",
    "            return\n",
<<<<<<< HEAD
    "        frac_done = self.step / self.learning_steps\n",
=======
    "        frac_done = (self.step) / self.learning_steps\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        lr = self.lr * (1 - frac_done)\n",
    "        for param_group in self.opt.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
<<<<<<< HEAD
    "    def optimize_normal(self):\n",
=======
    "    \n",
    "    def optimize_normal(self):\n",
    "        # NOTE assuming no gradient clipping\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        self._log_grad_norm()\n",
    "        self._anneal_lr()\n",
    "        self.opt.step()\n",
    "        for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "            update_ema(params, self.master_params, rate=rate)\n",
<<<<<<< HEAD
    "\n",
    "    def run_step(self, batch, cond):\n",
    "        self.forward_backward(batch, cond)\n",
    "        self.optimize_normal()\n",
    "\n",
    "    def forward_only(self, batch, cond):\n",
    "        with torch.no_grad():\n",
    "            zero_grad(self.model_params)\n",
    "            for i in range(0, batch.shape[0], self.microbatch):\n",
    "                micro = batch[i: i + self.microbatch]\n",
    "                micro_cond = {k: v[i: i + self.microbatch] for k, v in cond.items()}\n",
    "                t, weights = self.schedule_sampler.sample(micro.shape[0], device)\n",
    "                losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "                log_loss_dict(self.diffusion, t, {f\"eval_{k}\": v * weights for k, v in losses.items()})\n",
    "                self.eval_losses.append(losses['loss'].mean().item())\n",
    "\n",
    "    def forward_backward(self, batch, cond):\n",
    "        zero_grad(self.model_params)\n",
    "        for i in range(0, batch.shape[0], self.microbatch):\n",
    "            micro = batch[i: i + self.microbatch]\n",
    "            micro_cond = {k: v[i: i + self.microbatch] for k, v in cond.items()}\n",
    "            t, weights = self.schedule_sampler.sample(micro.shape[0], device)\n",
    "            losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "            loss = (losses[\"loss\"] * weights).mean()\n",
    "            log_loss_dict(self.diffusion, t, {k: v * weights for k, v in losses.items()})\n",
    "            loss.backward()\n",
    "            self.train_losses.append(loss.item())\n",
    "\n",
    "    def run_loop(self):\n",
    "        try:\n",
    "            with tqdm(total=self.learning_steps, desc=\"Training Progress\") as pbar:\n",
    "                while not self.learning_steps or self.step < self.learning_steps:\n",
    "                    batch, cond = next(self.data)\n",
    "                    self.run_step(batch, cond)\n",
    "                    if self.eval_data is not None and self.step % self.eval_interval == 0:\n",
    "                        batch_eval, cond_eval = next(self.eval_data)\n",
    "                        self.forward_only(batch_eval, cond_eval)\n",
    "#                         print('Eval on validation set')\n",
    "                    self.step += 1\n",
    "                    pbar.update(1)\n",
=======
    "     \n",
    "\n",
    "    def run_step(self, batch, cond):\n",
    "        # TODO implement this fn\n",
    "        self.forward_backward(batch, cond)\n",
    "        # NOTE assuming not using fp16 optimization\n",
    "        self.optimize_normal()\n",
    "        # TODO do this fn - logging\n",
    "        # self.log_step()\n",
    "\n",
    "    def forward_only(self, batch, cond):\n",
    "        with th.no_grad():\n",
    "            zero_grad(self.model_params)\n",
    "            for i in range(0, batch.shape[0], self.microbatch):\n",
    "                micro = batch[i: i + self.microbatch]#.to(device)\n",
    "                micro_cond = {\n",
    "                    k: v[i: i + self.microbatch]#.to(device)\n",
    "                    for k, v in cond.items()\n",
    "                }\n",
    "                last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "                t, weights = self.schedule_sampler.sample(micro.shape[0], device)\n",
    "\n",
    "                compute_losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "\n",
    "                # NOTE not using ddp - distributed training\n",
    "                with self.ddp_model.no_sync():\n",
    "                        losses = compute_losses()\n",
    "\n",
    "                log_loss_dict(\n",
    "                    self.diffusion, t, {f\"eval_{k}\": v * weights for k, v in losses.items()}\n",
    "                )\n",
    "\n",
    "\n",
    "    def forward_backward(self, batch, cond):\n",
    "        print(\"batch size\", batch.size())\n",
    "        zero_grad(self.model_params)\n",
    "        for i in range(0, batch.shape[0], self.microbatch):\n",
    "            micro = batch[i : i + self.microbatch] #.to(dist_util.dev())\n",
    "            micro_cond = {\n",
    "                k: v[i : i + self.microbatch]#.to(dist_util.dev())\n",
    "                for k, v in cond.items()\n",
    "            }\n",
    "            last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "            t, weights = self.schedule_sampler.sample(micro.shape[0],torch.device)# dist_util.dev())\n",
    "            # print(micro_cond.keys())\n",
    "            losses = self.diffusion.training_losses(self.ddp_model, micro, t, model_kwargs=micro_cond)\n",
    "\n",
    "            # NOTE not using ddp - distributed training\n",
    "            # with self.ddp_model.no_sync():\n",
    "\n",
    "            # NOTE losses datatype a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                #  Some mean or variance settings may also have other keys.\n",
    "            # losses = compute_losses()\n",
    "\n",
    "            print(\"losses\", losses)\n",
    "\n",
    "            # if isinstance(self.schedule_sampler, LossAwareSampler):\n",
    "            #     self.schedule_sampler.update_with_local_losses(\n",
    "            #         t, losses[\"loss\"].detach()\n",
    "            #     )\n",
    "\n",
    "            loss = (losses[\"loss\"] * weights).mean()\n",
    "            log_loss_dict(\n",
    "                self.diffusion, t, {k: v * weights for k, v in losses.items()}\n",
    "            )\n",
    "            # NOTE not using fp16 optimization \n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "    # NOTE not going to implement capacity to resume training     \n",
    "    # NOTE removed saving checkpoints \n",
    "    # NOTE removed some logging\n",
    "    def run_loop(self):\n",
    "        try:\n",
    "            while (\n",
    "                not self.learning_steps\n",
    "                or self.step < self.learning_steps\n",
    "            ):\n",
    "                batch, cond = next(self.data)\n",
    "                self.run_step(batch, cond)\n",
    "                if self.eval_data is not None and self.step % self.eval_interval == 0:\n",
    "                    batch_eval, cond_eval = next(self.eval_data)\n",
    "                    self.forward_only(batch_eval, cond_eval)\n",
    "                    print('eval on validation set')\n",
    "                self.step += 1\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        except StopIteration:\n",
    "            print(\"Data loader exhausted. Saving the model...\")\n",
    "\n",
    "        # Save the model after training is completed\n",
    "        if not os.path.exists('checkpoints'):\n",
    "            os.makedirs('checkpoints')\n",
    "        torch.save(self.model.state_dict(), 'checkpoints/trained_model.pth')\n",
    "        print(\"Model saved successfully.\")\n",
<<<<<<< HEAD
    "        \n",
    "        # Plotting the training and validation losses\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        if self.eval_losses:\n",
    "            plt.plot(self.eval_losses, label='Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss over Time')\n",
    "        plt.show()\n",
    "\n",
    "# Adjusting the learning steps and validation interval\n",
    "learning_steps = 800\n",
    "eval_interval = 1\n",
    "\n",
    "# Run training loop\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=iter(train_loader),\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_steps=learning_steps,\n",
    "        eval_data=iter(valid_loader),\n",
    "        eval_interval=eval_interval\n",
    "    ).run_loop()\n"
=======
    "    "
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate all the classes for training loop"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the noise schedule\n",
    "# # TODO consider other noise schedules, here take the simplifying assumption that we use linear noise schedule \n",
    "# # Linear schedule from Ho et al, extended to work for any number of\n",
    "# # diffusion steps.\n",
    "\n",
    "# scale = 1000 / num_diffusion_timesteps\n",
    "# beta_start = scale * 0.0001\n",
    "# beta_end = scale * 0.02\n",
    "# betas = np.linspace(\n",
    "#     beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    "# )\n",
    "\n",
    "# # NOTE we assume NO timestep respacing \n",
    "# # TODO instantiate the diffusion model & transformer and save it (?)\n",
    "\n",
    "# diffusion = GaussianDiffusion(betas=betas)\n",
    "\n",
    "# # TODO specify correct dimensions!\n",
    "# # Note embedding size is 128\n",
    "# model = TransformerNetModel(vocab_size=vocab_size, input_dims=embedding_dim, hidden_t_dim=hidden_dim, output_dims=output_dims)\n",
    "\n",
    "# # model.to(device) \n",
    "\n",
    "# pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "# # TODO add to logger\n",
    "# print(f'### The parameter count is {pytorch_total_params}')"
=======
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer self.hidden_size 768\n",
      "### The parameter count is 110184634\n"
     ]
    }
   ],
   "source": [
    "# Define the noise schedule\n",
    "# TODO consider other noise schedules, here take the simplifying assumption that we use linear noise schedule \n",
    "# Linear schedule from Ho et al, extended to work for any number of\n",
    "# diffusion steps.\n",
    "\n",
    "scale = 1000 / num_diffusion_timesteps\n",
    "beta_start = scale * 0.0001\n",
    "beta_end = scale * 0.02\n",
    "betas = np.linspace(\n",
    "    beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    ")\n",
    "\n",
    "# NOTE we assume NO timestep respacing \n",
    "# TODO instantiate the diffusion model & transformer and save it (?)\n",
    "\n",
    "diffusion = GaussianDiffusion(betas=betas)\n",
    "\n",
    "# TODO specify correct dimensions!\n",
    "# Note embedding size is 128\n",
    "model = TransformerNetModel(vocab_size=vocab_size, input_dims=embedding_dim, hidden_t_dim=hidden_dim, output_dims=output_dims)\n",
    "\n",
    "# model.to(device) \n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "# TODO add to logger\n",
    "print(f'### The parameter count is {pytorch_total_params}')"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for training\n",
    "\n",
    "Note the implementation details in DiffuSeq (first version) is\n",
    "\"The maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2000\n",
    "and a square-root noise schedule.\"\n",
    "\n",
    "How is it different in v2 or other papers?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# # Run training loop\n",
    "# TrainLoop(\n",
    "#         model=model,\n",
    "#         diffusion=diffusion,\n",
    "#         data=data, # TODO load the text data\n",
    "#         batch_size=batch_size, # TODO set hyperparams from args like args.batch_size\n",
    "#         # microbatch=microbatch,\n",
    "#         lr=lr,\n",
    "#         ema_rate=ema_rate,\n",
    "#         # log_interval=args.log_interval,\n",
    "#         weight_decay=weight_decay,\n",
    "#         learning_steps=learning_steps,\n",
    "#         # checkpoint_path=args.checkpoint_path,\n",
    "#         # gradient_clipping=args.gradient_clipping,\n",
    "#         # eval_data=data_valid,\n",
    "#         # eval_interval=args.eval_interval\n",
    "#     ).run_loop()\n"
=======
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size torch.Size([19, 128, 128])\n",
      "x_start size torch.Size([19, 128, 128])\n",
      "x_start_mean size torch.Size([19, 128, 768])\n",
      "Forward step\n",
      "x: an [N x C x ...] Tensor of inputs size: torch.Size([19, 128, 768])\n",
      "emb_x.shape, emb_t.shape, self.position_embeddings\n",
      "torch.Size([19, 128, 768]) torch.Size([19, 768]) Embedding(512, 768)\n",
      "input_trans_hideden_states.shape torch.Size([19, 128, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[300], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Run training loop\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mTrainLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# TODO load the text data\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# TODO set hyperparams from args like args.batch_size\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# microbatch=microbatch,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# log_interval=args.log_interval,\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# checkpoint_path=args.checkpoint_path,\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# gradient_clipping=args.gradient_clipping,\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# eval_data=data_valid,\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# eval_interval=args.eval_interval\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[298], line 164\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_steps\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_steps\n\u001b[0;32m    162\u001b[0m ):\n\u001b[0;32m    163\u001b[0m     batch, cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    166\u001b[0m         batch_eval, cond_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_data)\n",
      "Cell \u001b[1;32mIn[298], line 89\u001b[0m, in \u001b[0;36mTrainLoop.run_step\u001b[1;34m(self, batch, cond)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, cond):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# TODO implement this fn\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# NOTE assuming not using fp16 optimization\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_normal()\n",
      "Cell \u001b[1;32mIn[298], line 130\u001b[0m, in \u001b[0;36mTrainLoop.forward_backward\u001b[1;34m(self, batch, cond)\u001b[0m\n\u001b[0;32m    128\u001b[0m t, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule_sampler\u001b[38;5;241m.\u001b[39msample(micro\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],torch\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;66;03m# dist_util.dev())\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# print(micro_cond.keys())\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmicro_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# NOTE not using ddp - distributed training\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# with self.ddp_model.no_sync():\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# NOTE losses datatype a dict with the key \"loss\" containing a tensor of shape [N].\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m#  Some mean or variance settings may also have other keys.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# losses = compute_losses()\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m, losses)\n",
      "Cell \u001b[1;32mIn[295], line 49\u001b[0m, in \u001b[0;36mGaussianDiffusion.training_losses\u001b[1;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_losses_seq2seq(model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[295], line 473\u001b[0m, in \u001b[0;36mGaussianDiffusion.training_losses_seq2seq\u001b[1;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[0;32m    470\u001b[0m out_mean, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_mean_variance(x_start, th\u001b[38;5;241m.\u001b[39mLongTensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(x_start\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m    471\u001b[0m tT_loss \u001b[38;5;241m=\u001b[39m  mean_flat(out_mean \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 473\u001b[0m decoder_nll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_discrete_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_x\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# embedding regularization\u001b[39;00m\n\u001b[0;32m    474\u001b[0m terms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnll\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask\u001b[38;5;241m=\u001b[39minput_ids_mask, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, t\u001b[38;5;241m=\u001b[39mt) \u001b[38;5;66;03m# x_0->model_out_x_start\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# assert (model.lm_head.weight == model.word_embedding.weight).all()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[295], line 386\u001b[0m, in \u001b[0;36mGaussianDiffusion._token_discrete_loss\u001b[1;34m(self, x_t, get_logits, input_ids, mask, truncate, t)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# print(logits.shape)\u001b[39;00m\n\u001b[0;32m    385\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 386\u001b[0m decoder_nll \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     decoder_nll \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m mask\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI4E\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI4E\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI4E\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI4E\\lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Run training loop\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data, # TODO load the text data\n",
    "        batch_size=batch_size, # TODO set hyperparams from args like args.batch_size\n",
    "        # microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        # log_interval=args.log_interval,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_steps=learning_steps,\n",
    "        # checkpoint_path=args.checkpoint_path,\n",
    "        # gradient_clipping=args.gradient_clipping,\n",
    "        # eval_data=data_valid,\n",
    "        # eval_interval=args.eval_interval\n",
    "    ).run_loop()\n"
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached StopIteration Exception. Training completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sample output using the forward step of the trained model."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# # Ensure the device is set correctly\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize the model\n",
    "# model = TransformerNetModel(\n",
    "#     vocab_size=vocab_size, \n",
    "#     input_dims=embedding_dim, \n",
    "#     hidden_t_dim=hidden_dim, \n",
    "#     output_dims=output_dims\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# # Load the trained model weights\n",
    "# model.load_state_dict(torch.load('checkpoints/trained_model.pth', map_location=device))\n",
    "# model.eval()\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# def generate_text(model, tokenizer, prompt, max_length=128, num_timesteps=2000):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Tokenize the input prompt\n",
    "#         input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "#         print(\"Input IDs:\", input_ids)\n",
    "\n",
    "#         # Get the embeddings for the input prompt\n",
    "#         input_embeds = model.word_embedding(input_ids).to(device)\n",
    "#         print(\"Input Embeddings:\", input_embeds)\n",
    "\n",
    "#         # Initialize noise\n",
    "#         noise = torch.randn_like(input_embeds).to(device)\n",
    "#         print(\"Noise:\", noise)\n",
    "\n",
    "#         # Set up the diffusion process\n",
    "#         diffusion = GaussianDiffusion(betas=np.linspace(1e-4, 0.02, num_timesteps))\n",
    "\n",
    "#         # Sample from the model using p_sample_loop\n",
    "#         samples = diffusion.p_sample_loop(\n",
    "#             model=model,\n",
    "#             shape=input_embeds.shape,\n",
    "#             noise=noise,\n",
    "#             device=device,\n",
    "#             progress=True,\n",
    "#             clamp_step=None  # Set this to a specific value if needed\n",
    "#         )\n",
    "\n",
    "#         # Convert the generated embeddings back to tokens\n",
    "#         final_sample = samples[-1].to(device)\n",
    "#         print(\"Final Sample Shape:\", final_sample.shape)\n",
    "        \n",
    "#         generated_ids = model.lm_head(final_sample).argmax(dim=-1)\n",
    "#         print(\"Generated IDs:\", generated_ids.squeeze().tolist())\n",
    "        \n",
    "#         generated_text = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "#         return generated_text\n",
    "\n",
    "# # Function to sample a few inputs from the training set\n",
    "# def evaluate_on_training_samples(tokenizer, model, num_samples=5):\n",
    "#     # Load a few samples from the training set\n",
    "#     train_samples = train_data['src'][:num_samples]\n",
    "    \n",
    "#     for i, sample in enumerate(train_samples):\n",
    "#         generated_response = generate_text(model, tokenizer, sample)\n",
    "#         print(f\"Sample {i+1} Prompt: {sample}\")\n",
    "#         print(f\"Sample {i+1} Generated Response: {generated_response}\\n\")\n",
    "\n",
    "# # Evaluate on a few training samples\n",
    "# evaluate_on_training_samples(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:12<00:00, 158.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2054, 2003, 1996, 3007, 1997, 2660, 1029,  102]],\n",
      "       device='cuda:0')\n",
      "Output IDs: tensor([[ 5880, 11136, 17507,  5142,  2893, 19068, 25022, 14719,  5249]],\n",
      "       device='cuda:0')\n",
      "Prompt: What is the capital of Australia?\n",
      "Generated Response: ##water pupil wakes concern gettingxed ci mir woods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
=======
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerNetModel(\n",
    "    vocab_size=vocab_size, \n",
    "    input_dims=embedding_dim, \n",
    "    hidden_t_dim=hidden_dim, \n",
    "    output_dims=output_dims\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('checkpoints/trained_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
<<<<<<< HEAD
    "def temperature_sampling(logits, temperature):\n",
    "    logits = logits / temperature\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=128, num_timesteps=2000, temperature=1.0):\n",
=======
    "def generate_text(model, tokenizer, prompt, max_length=128, num_timesteps=2000):\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input prompt\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        # Get the embeddings for the input prompt\n",
    "        input_embeds = model.word_embedding(input_ids).to(device)\n",
    "\n",
    "        # Initialize noise\n",
    "        noise = torch.randn_like(input_embeds).to(device)\n",
    "\n",
    "        # Set up the diffusion process\n",
    "        diffusion = GaussianDiffusion(betas=np.linspace(1e-4, 0.02, num_timesteps))\n",
    "\n",
    "        # Sample from the model using p_sample_loop\n",
    "        samples = diffusion.p_sample_loop(\n",
    "            model=model,\n",
    "            shape=input_embeds.shape,\n",
    "            noise=noise,\n",
    "            device=device,\n",
    "            progress=True,\n",
    "            clamp_step=None  # Set this to a specific value if needed\n",
    "        )\n",
    "\n",
    "        # Convert the generated embeddings back to tokens\n",
    "        generated_ids = model.lm_head(samples[-1].to(device))\n",
<<<<<<< HEAD
    "\n",
    "        # Apply temperature sampling\n",
    "        generated_ids = generated_ids.view(-1, generated_ids.size(-1))  # Reshape to (batch_size * seq_len, vocab_size)\n",
    "        sampled_ids = temperature_sampling(generated_ids, temperature)\n",
    "        sampled_ids = sampled_ids.view(1, -1)  # Reshape back to (1, seq_len)\n",
    "        \n",
    "        generated_text = tokenizer.decode(sampled_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        # Print input IDs and output IDs\n",
    "        print(f\"Input IDs: {input_ids}\")\n",
    "        print(f\"Output IDs: {sampled_ids}\")\n",
    "\n",
=======
    "        generated_text = tokenizer.decode(generated_ids.argmax(dim=-1).squeeze().tolist(), skip_special_tokens=True)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "        return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What is the capital of Australia?\"\n",
<<<<<<< HEAD
    "generated_response = generate_text(model, tokenizer, prompt, temperature=0.7)\n",
=======
    "generated_response = generate_text(model, tokenizer, prompt)\n",
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Response: {generated_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference step (sampling / generation part)\n",
    "\n",
    "Once the training completed, we can start the inference step and get cross validation accuracy."
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> 2e43c16eded0660e0a4bbbfff41261d058ebb8e1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
