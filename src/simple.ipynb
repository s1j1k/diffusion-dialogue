{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model for Open Dialogue\n",
    "\n",
    "Simplified version\n",
    "\n",
    "References\n",
    "- DiffuSeq (cited below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{gong2022diffuseq,\n",
    "  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  booktitle = {International Conference on Learning Representations, ICLR},\n",
    "  title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},\n",
    "  year = 2023\n",
    "}\n",
    "\n",
    "@article{gong2023diffuseqv2,\n",
    "  title={DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models},\n",
    "  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  journal={arXiv preprint arXiv:2310.05793},\n",
    "  year={2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "We should use BERT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note that, in DiffuSeq, a model file is created to store all training progress, configuration etc. (in bash format poitning to raw files?)\n",
    "\n",
    "- denoise rate ?\n",
    "- using updates in v2 diffuseq took it from 2 days -> 11 hr learning time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data\n",
    "What are we trying to achieve in summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.sh in diffuseq\n",
    "#  python -m torch.distributed.launch --nproc_per_node=4 --master_port=12233 --use_env run_train.py \\\n",
    "# --diff_steps 2000 \\\n",
    "# --lr 0.0001 \\\n",
    "# --learning_steps 50000 \\\n",
    "# --save_interval 10000 \\\n",
    "# --seed 102 \\\n",
    "# --noise_schedule sqrt \\\n",
    "# --hidden_dim 128 \\\n",
    "# --bsz 2048 \\\n",
    "# --dataset test \\\n",
    "# --data_dir '/Users/sallykalumba/dev/diffusion-dialogue/DiffuSeq/datasets/CommonsenseConversation' \\\n",
    "# --vocab bert \\\n",
    "# --seq_len 128 \\\n",
    "# --schedule_sampler lossaware \\\n",
    "# --notes test-qqp\n",
    "\n",
    "# saves model & runs tarin.py with arguments\n",
    "\n",
    "# TODO run on GPU\n",
    "\n",
    "# train.py \n",
    "# parses arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# in diffuseq:\n",
    "# uses a special helper function for loading text data\n",
    "\n",
    "# data = load_data_text(\n",
    "#         batch_size=args.batch_size,\n",
    "#         seq_len=args.seq_len,\n",
    "#         data_args = args,\n",
    "#         loaded_vocab=tokenizer,\n",
    "#         model_emb=model_weight # use model's weights as init\n",
    "#     )\n",
    "#     next(data)\n",
    "\n",
    "#     data_valid = load_data_text(\n",
    "#         batch_size=args.batch_size,\n",
    "#         seq_len=args.seq_len,\n",
    "#         data_args=args,\n",
    "#         split='valid',\n",
    "#         deterministic=True,\n",
    "#         loaded_vocab=tokenizer,\n",
    "#         model_emb=model_weight # using the same embedding wight with tranining data\n",
    "#     )\n",
    "\n",
    "\n",
    "# prints the size of vocab \n",
    "# then creates model & diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen dataset must be placed into the below format,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "# referring to load_data_text\n",
    "\n",
    "\"\"\"\n",
    "For a dataset, create a generator over (seqs, kwargs) pairs.\n",
    "\n",
    "Each seq is an (bsz, len, h) float tensor, and the kwargs dict contains zero or\n",
    "more keys, each of which map to a batched Tensor of their own.\n",
    "The kwargs dict can be used for some meta information.\n",
    "\n",
    ":param batch_size: the batch size of each returned pair.\n",
    ":param seq_len: the max sequence length (one-side).\n",
    ":param deterministic: if True, yield results in a deterministic order.\n",
    ":param data_args: including dataset directory, num of dataset, basic settings, etc.\n",
    ":param model_emb: loaded word embeddings.\n",
    ":param loaded_vocab: loaded word vocabs.\n",
    ":param loop: loop to get batch data or not.\n",
    "\"\"\"\n",
    "\n",
    "# load text data\n",
    "\n",
    "\n",
    "# training_data = get_corpus(data_args, seq_len, split=split, loaded_vocab=loaded_vocab)\n",
    "\n",
    "# return an iterative data loader\n",
    "\n",
    "# get a tokenized text data from the training data set \n",
    "\n",
    "\n",
    "# if split == 'train':\n",
    "#         print('### Loading form the TRAIN set...')\n",
    "#         path = f'{data_args.data_dir}/train.jsonl'\n",
    "#     elif split == 'valid':\n",
    "#         print('### Loading form the VALID set...')\n",
    "#         path = f'{data_args.data_dir}/valid.jsonl'\n",
    "#     elif split == 'test':\n",
    "#         print('### Loading form the TEST set...')\n",
    "#         path = f'{data_args.data_dir}/test.jsonl'\n",
    "#     else:\n",
    "#         assert False, \"invalid split for dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = \"./datasets/sample\"\n",
    "path = f'{data_dir}/train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data in training data json file \n",
    "# TODO do this in a different way \n",
    "# FIXME what is the .strip() used for ?\n",
    "sentence_lst = {'src':[], 'trg': []}\n",
    "with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO tokenize datasets\n",
    "\n",
    "# TODO check the literature to see what the actual steps we are doing here\n",
    "\n",
    "\n",
    "# get tokenizer\n",
    "# tokenizer = load_tokenizer(args)\n",
    "\n",
    "#  loaded_vocab=tokenizer,\n",
    "\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#         input_id_x = vocab_dict.encode_token(examples['src'])\n",
    "#         input_id_y = vocab_dict.encode_token(examples['trg'])\n",
    "#         result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "#         return result_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FIXME where to get the loaded vocab?\n",
    "\n",
    "# TODO get a dataseet from dict\n",
    "\n",
    "# FIXME vocab dict is loaded from file \n",
    "\n",
    "\n",
    "\n",
    "#     tokenized_datasets = raw_datasets.map(\n",
    "#         tokenize_function,\n",
    "#         batched=True,\n",
    "#         num_proc=4,\n",
    "#         remove_columns=['src', 'trg'],\n",
    "#         load_from_cache_file=True,\n",
    "#         desc=\"Running tokenizer on dataset\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallykalumba/dev/diffusion-dialogue/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sallykalumba/dev/diffusion-dialogue/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Diffusion-LM (Li et al., 2022)\n",
    "# embedding function $$EMB(w)$$ to map the discrete text $$w$$ into a continuous space. -->\n",
    "\n",
    "# TODO make a new tokenizer\n",
    "\n",
    "\"\"\"\n",
    "    Load tokenizer from bert config or defined BPE vocab dict\n",
    "    \"\"\"\n",
    "\n",
    "# TODO explore using other tokenizer, for this simplified model, just use BERT\n",
    "# in DiffuSq have the option of a custom tokenizer, and the chosne tokenizer is saved\n",
    "\n",
    "# TODO what is the difference between AutoTokenizer and BertTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args.config_name)\n",
    "#             self.tokenizer = tokenizer\n",
    "#             self.sep_token_id = tokenizer.sep_token_id\n",
    "#             self.pad_token_id = tokenizer.pad_token_id\n",
    "#             # save\n",
    "#             tokenizer.save_pretrained(args.checkpoint_path)\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "# # Load pre-trained tokenizer and model\n",
    "\n",
    "# # Bert == loaded vocab\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
