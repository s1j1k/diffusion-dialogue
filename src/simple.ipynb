{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model for Open Dialogue\n",
    "\n",
    "Simplified version\n",
    "\n",
    "References\n",
    "- DiffuSeq (cited below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "\n",
    "@inproceedings{gong2022diffuseq,\n",
    "  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  booktitle = {International Conference on Learning Representations, ICLR},\n",
    "  title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},\n",
    "  year = 2023\n",
    "}\n",
    "\n",
    "@article{gong2023diffuseqv2,\n",
    "  title={DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models},\n",
    "  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  journal={arXiv preprint arXiv:2310.05793},\n",
    "  year={2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adapt from other codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Use Commonsense Conversation dataset (from Reddit)\n",
    "\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note that, in DiffuSeq, a model file is created to store all training progress, configuration etc. (in bash format poitning to raw files?)\n",
    "\n",
    "- denoise rate ?\n",
    "- using updates in v2 diffuseq took it from 2 days -> 11 hr learning time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer\n",
    "\n",
    "For simplicity choose BERT rather than custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# use GPU if available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallykalumba/dev/diffusion-dialogue/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/sallykalumba/dev/diffusion-dialogue/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer from bert\n",
    "# TODO check if this is the best tokenizer for Commonsense Conversation dataset\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# sep_token_id = tokenizer.sep_token_id\n",
    "# pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion-LM (Li et al., 2022):\n",
    "\n",
    " embedding function $EMB(w)$ to map the discrete text $w$ into a continuous space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model embeddings\n",
    "```\n",
    "model_weight, tokenizer = load_model_emb(args, tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the implementation details in DiffuSeq (first version) is\n",
    "\"The maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2000\n",
    "and a square-root noise schedule.\"\n",
    "\n",
    "How is it different in v2 or other papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8701,  0.4414, -0.3765,  ..., -0.8231, -4.0339, -0.7709],\n",
       "        [ 0.7203,  1.2298, -0.1098,  ...,  0.3902,  0.7161, -1.8676],\n",
       "        [ 0.5635, -0.0068,  0.4906,  ..., -1.5021,  0.8436,  0.1401],\n",
       "        ...,\n",
       "        [-1.9333,  0.7976, -2.9892,  ..., -0.7466, -2.1189, -0.7736],\n",
       "        [ 0.0074,  1.9049, -0.9545,  ..., -0.2393,  1.0275, -0.2788],\n",
       "        [-0.6450,  0.6781, -2.1758,  ..., -1.1580,  0.6388, -0.7978]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose hidden_dim\n",
    "\n",
    "# choose embedding dimension\n",
    "d=128 \n",
    "# TODO choose embedding dimension suitable to the task\n",
    "\n",
    "model = torch.nn.Embedding(tokenizer.vocab_size, 128)\n",
    "\n",
    "# initialize random embeddings\n",
    "torch.nn.init.normal_(model.weight)\n",
    "\n",
    "# torch.save(model.state_dict(), path_save)\n",
    "# os.sync() FIXME required?\n",
    "\n",
    "# FIXME need to implement saving and reloading?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the text data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data in training data json file \n",
    "# TODO do this in a different way \n",
    "# FIXME what is the .strip() used for specifically?\n",
    "\n",
    "import json\n",
    "\n",
    "data_dir = \"./datasets/sample\"\n",
    "path = f'{data_dir}/train.jsonl'\n",
    "\n",
    "sentence_lst = {'src':[], 'trg': []}\n",
    "with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "\n",
    "\n",
    "# TODO use pandas to load faster? any other package can just load json directly rather than row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize dataset\n",
    "\n",
    "# NOTE custom toknizer is of type dict in DiffuSeq\n",
    "\n",
    "input_id_x = tokenizer(sentence_lst['src'], add_special_tokens=True)['input_ids']\n",
    "input_id_y = tokenizer(sentence_lst['trg'], add_special_tokens=True)['input_ids']\n",
    "tokenized_dataset = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "# TODO use batching & multiple processees to make more efficient, use map function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO crate a DATALOADER\n",
    "\n",
    "#  if split != 'test':\n",
    "#         sampler = DistributedSampler(dataset)\n",
    "#         data_loader = DataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=batch_size,  # 20,\n",
    "#             # drop_last=True,\n",
    "#             sampler=sampler,\n",
    "#             # shuffle=not deterministic,\n",
    "#             num_workers=4,\n",
    "#         )\n",
    "#     else:\n",
    "#         data_loader = DataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=batch_size,  # 20,\n",
    "#             # drop_last=True,\n",
    "#             # sampler=sampler,\n",
    "#             shuffle=not deterministic,\n",
    "#             num_workers=4,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Is this data in continuous space???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=20,  # TODO choose an appropriate batch size ?\n",
    "    # shuffle=, # TODO choose if you want to shuffle\n",
    "    # num_workers=4, # TODO use multiple workers throughout script?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the model and diffusion\n",
    "```\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "        **args_to_dict(args, load_defaults_config().keys())\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create schedule sampler? is it required?\n",
    "\n",
    "run the training loop according to batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model = TransformerNetModel(\n",
    "#         input_dims=hidden_dim,\n",
    "#         output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "#         hidden_t_dim=hidden_t_dim,\n",
    "#         dropout=dropout,\n",
    "#         config_name=config_name,\n",
    "#         vocab_size=vocab_size,\n",
    "#         init_pretrained=use_plm_init\n",
    "#     )\n",
    "\n",
    "#     betas = gd.get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "\n",
    "#     if not timestep_respacing:\n",
    "#         timestep_respacing = [diffusion_steps]\n",
    "\n",
    "#     diffusion = SpacedDiffusion(\n",
    "#         use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "#         betas=betas,\n",
    "#         rescale_timesteps=rescale_timesteps,\n",
    "#         predict_xstart=predict_xstart,\n",
    "#         learn_sigmas = learn_sigma,\n",
    "#         sigma_small = sigma_small,\n",
    "#         use_kl = use_kl,\n",
    "#         rescale_learned_sigmas=rescale_learned_sigmas\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME need to define TransformerNetModel\n",
    "#  The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "# Adapted from diffuSeq\n",
    "\n",
    "# TODO code the transformer from scratch\n",
    "# TODO design the transformer from scratch\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Config from DiffuSeq\n",
    "  #     self,\n",
    "    #     input_dims,\n",
    "    #     output_dims,\n",
    "    #     hidden_t_dim,\n",
    "    #     dropout=0,\n",
    "    #     config=None,\n",
    "    #     config_name='bert-base-uncased',\n",
    "    #     vocab_size=None,\n",
    "    #     init_pretrained='no',\n",
    "    #     logits_mode=1,\n",
    "    # ):\n",
    "\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "     def __init__(self, vocab_size, input_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        # FIXME just using the default config not a param\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        # FIXME add in this to the params - config, defining here as hard-coded config=\n",
    "        # FIXME set to an actual value\n",
    "        # config.hidden_size = what?\n",
    "        # FIXME set to an actual value\n",
    "        config.hidden_dropout_prob = 0\n",
    "        # then pass this to the BertModel config=config\n",
    "\n",
    "        # FIXME specify input_dims\n",
    "        self.input_dims = input_dims\n",
    "        \n",
    "\n",
    "        # self.hidden_t_dim = hidden_t_dim\n",
    "        # self.output_dims = output_dims\n",
    "        # self.dropout = dropout\n",
    "        # self.logits_mode = logits_mode\n",
    "\n",
    "        # TODO check this gets assigned by default BERT config\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # TODO add work embeddings\n",
    "        # FIXME vocab_size is define way up above\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "\n",
    "        # FIXME what is LM head specifying (?)\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "\n",
    "        with th.no_grad(): # disable gradient calculations\n",
    "            # FIXME describe what are we doing here\n",
    "            self.lm_head.weight = self.word_embedding.weight \n",
    "\n",
    "\n",
    "        # FIXME what is this for\n",
    "        # time_embed_dim = hidden_t_dim * 4\n",
    "        # self.time_embed = nn.Sequential(\n",
    "        #     linear(hidden_t_dim, time_embed_dim),\n",
    "        #     SiLU(),\n",
    "        #     linear(time_embed_dim, config.hidden_size),\n",
    "        # )\n",
    "\n",
    "\n",
    "        #  if self.input_dims != config.hidden_size:\n",
    "        # self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "        #                                   nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "\n",
    "\n",
    "        # print('initializing from pretrained bert...')\n",
    "        #     print(config)\n",
    "\n",
    "        # FIXME why is this temporary \n",
    "        temp_bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "            # with th.no_grad():\n",
    "            #     self.lm_head.weight = self.word_embedding.weight\n",
    "            # # self.lm_head.weight.requires_grad = False\n",
    "            # # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "        # TODO explain what is happening\n",
    "        self.input_transformers = temp_bert.encoder\n",
    "        # TODO explain what is doing\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "        self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "     \n",
    "        del temp_bert.embeddings\n",
    "        del temp_bert.pooler\n",
    "\n",
    "        # FIXME When does this get used\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # FIXME what is happening here\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "\n",
    "          \n",
    "\n",
    "  # FIXME what is the difference btween BertModel, BertConfig, BertTokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
