{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model for Open Dialogue\n",
    "\n",
    "Simplified version\n",
    "\n",
    "References\n",
    "- DiffuSeq (cited below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from:\n",
    "\n",
    "@inproceedings{gong2022diffuseq,\n",
    "  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  booktitle = {International Conference on Learning Representations, ICLR},\n",
    "  title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},\n",
    "  year = 2023\n",
    "}\n",
    "\n",
    "@article{gong2023diffuseqv2,\n",
    "  title={DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models},\n",
    "  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},\n",
    "  journal={arXiv preprint arXiv:2310.05793},\n",
    "  year={2023}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adapt from other codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Use Commonsense Conversation dataset (from Reddit)\n",
    "\n",
    "\n",
    "in diffuseq text_datasets.py some steps to load the dataset itself\n",
    "\n",
    "- [ ] prepare datasets for training and validation in the format (stored as jsonl file?)\n",
    "```\n",
    "{\"src\": \"\", \"train\": \"\"}\n",
    "```\n",
    "\n",
    "- word embeddings (to be loaded?)\n",
    "- use a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note that, in DiffuSeq, a model file is created to store all training progress, configuration etc. (in bash format poitning to raw files?)\n",
    "\n",
    "- denoise rate ?\n",
    "- using updates in v2 diffuseq took it from 2 days -> 11 hr learning time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer\n",
    "\n",
    "For simplicity choose BERT rather than custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# use GPU if available\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer from bert\n",
    "# TODO check if this is the best tokenizer for Commonsense Conversation dataset\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# sep_token_id = tokenizer.sep_token_id\n",
    "# pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion-LM (Li et al., 2022):\n",
    "\n",
    " embedding function $EMB(w)$ to map the discrete text $w$ into a continuous space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model embeddings\n",
    "```\n",
    "model_weight, tokenizer = load_model_emb(args, tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the implementation details in DiffuSeq (first version) is\n",
    "\"The maximum sequence length is 128, with embedding dimension d = 128, diffusion steps T = 2000\n",
    "and a square-root noise schedule.\"\n",
    "\n",
    "How is it different in v2 or other papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.1163,  0.2913,  1.5749,  ...,  1.9148,  0.1161,  0.9461],\n",
       "        [-0.0383, -0.7269, -0.6582,  ...,  0.5458,  0.7316,  0.9242],\n",
       "        [-1.5244,  0.3114,  1.8426,  ..., -0.0750,  1.1773, -0.5259],\n",
       "        ...,\n",
       "        [ 0.8354, -0.5204, -1.5170,  ..., -1.8002, -0.6871, -0.0698],\n",
       "        [-0.4749, -0.5177, -0.6251,  ...,  0.1664, -0.1985, -1.5135],\n",
       "        [-0.2264,  0.9835,  0.8102,  ...,  0.3530, -0.3836,  0.0236]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose hidden_dim\n",
    "\n",
    "# choose embedding dimension\n",
    "d=128 \n",
    "# TODO choose embedding dimension suitable to the task\n",
    "\n",
    "model = torch.nn.Embedding(tokenizer.vocab_size, 128)\n",
    "\n",
    "# initialize random embeddings\n",
    "torch.nn.init.normal_(model.weight)\n",
    "\n",
    "# torch.save(model.state_dict(), path_save)\n",
    "# os.sync() FIXME required?\n",
    "\n",
    "# FIXME need to implement saving and reloading?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the text data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data in training data json file \n",
    "# TODO do this in a different way \n",
    "# FIXME what is the .strip() used for specifically?\n",
    "\n",
    "\n",
    "# TODO load actual dataset from Amazon\n",
    "\n",
    "import json\n",
    "\n",
    "data_dir = \"./datasets/sample\"\n",
    "path = f'{data_dir}/train.jsonl'\n",
    "\n",
    "sentence_lst = {'src':[], 'trg': []}\n",
    "with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "\n",
    "\n",
    "# TODO use pandas to load faster? any other package can just load json directly rather than row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize dataset\n",
    "\n",
    "# NOTE custom toknizer is of type dict in DiffuSeq\n",
    "\n",
    "input_id_x = tokenizer(sentence_lst['src'], add_special_tokens=True)['input_ids']\n",
    "input_id_y = tokenizer(sentence_lst['trg'], add_special_tokens=True)['input_ids']\n",
    "tokenized_dataset = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "# TODO use batching & multiple processees to make more efficient, use map function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO crate a DATALOADER\n",
    "\n",
    "#  if split != 'test':\n",
    "#         sampler = DistributedSampler(dataset)\n",
    "#         data_loader = DataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=batch_size,  # 20,\n",
    "#             # drop_last=True,\n",
    "#             sampler=sampler,\n",
    "#             # shuffle=not deterministic,\n",
    "#             num_workers=4,\n",
    "#         )\n",
    "#     else:\n",
    "#         data_loader = DataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=batch_size,  # 20,\n",
    "#             # drop_last=True,\n",
    "#             # sampler=sampler,\n",
    "#             shuffle=not deterministic,\n",
    "#             num_workers=4,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Is this data in continuous space???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=20,  # TODO choose an appropriate batch size ?\n",
    "    # shuffle=, # TODO choose if you want to shuffle\n",
    "    # num_workers=4, # TODO use multiple workers throughout script?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the model and diffusion\n",
    "```\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "        **args_to_dict(args, load_defaults_config().keys())\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create schedule sampler? is it required?\n",
    "\n",
    "run the training loop according to batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model = TransformerNetModel(\n",
    "#         input_dims=hidden_dim,\n",
    "#         output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "#         hidden_t_dim=hidden_t_dim,\n",
    "#         dropout=dropout,\n",
    "#         config_name=config_name,\n",
    "#         vocab_size=vocab_size,\n",
    "#         init_pretrained=use_plm_init\n",
    "#     )\n",
    "\n",
    "#     betas = gd.get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "\n",
    "#     if not timestep_respacing:\n",
    "#         timestep_respacing = [diffusion_steps]\n",
    "\n",
    "#     diffusion = SpacedDiffusion(\n",
    "#         use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "#         betas=betas,\n",
    "#         rescale_timesteps=rescale_timesteps,\n",
    "#         predict_xstart=predict_xstart,\n",
    "#         learn_sigmas = learn_sigma,\n",
    "#         sigma_small = sigma_small,\n",
    "#         use_kl = use_kl,\n",
    "#         rescale_learned_sigmas=rescale_learned_sigmas\n",
    "#     )\n",
    "\n",
    "# FIXME need to implement a way to save the model progress so that\n",
    "# trained model can be loaded later for assessment purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME need to define TransformerNetModel\n",
    "#  The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "# Adapted from diffuSeq\n",
    "\n",
    "# TODO code the transformer from scratch\n",
    "# TODO design the transformer from scratch\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Config from DiffuSeq\n",
    "  #     self,\n",
    "    #     input_dims,\n",
    "    #     output_dims,\n",
    "    #     hidden_t_dim,\n",
    "    #     dropout=0,\n",
    "    #     config=None,\n",
    "    #     config_name='bert-base-uncased',\n",
    "    #     vocab_size=None,\n",
    "    #     init_pretrained='no',\n",
    "    #     logits_mode=1,\n",
    "    # ):\n",
    "\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "     def __init__(self, vocab_size, input_dims, hidden_t_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # FIXME just using the default config not a param\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        # FIXME add in this to the params - config, defining here as hard-coded config=\n",
    "        # FIXME set to an actual value\n",
    "        # config.hidden_size = what?\n",
    "        # FIXME set to an actual value\n",
    "        config.hidden_dropout_prob = 0\n",
    "        # then pass this to the BertModel config=config\n",
    "\n",
    "        # FIXME specify input_dims\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        # TODO include condfiguration for hidden_t_dim -> time embeddings hidden dimensions\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        # self.output_dims = output_dims\n",
    "        # self.dropout = dropout\n",
    "        # TODO check this gets assigned by default BERT config\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # TODO add work embeddings\n",
    "        # FIXME vocab_size is define way up above\n",
    "\n",
    "        # Generate logits for hidden representation\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "\n",
    "        # Time embeddings\n",
    "        # FIXME investigate this implementation\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(hidden_t_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "\n",
    "        # FIXME What does this do\n",
    "        # Function to deal with having a hidden size not equal to input size, project to hidden size (?)\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "                                            nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "\n",
    "        # FIXME why is this temporary \n",
    "        temp_bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "        self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "       \n",
    "       # FIXME why do we do this 2 times????\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "        # self.lm_head.weight.requires_grad = False\n",
    "        # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "        # TODO explain what is happening\n",
    "        self.input_transformers = temp_bert.encoder\n",
    "        # TODO explain what is doing\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "        self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "     \n",
    "        del temp_bert.embeddings\n",
    "        del temp_bert.pooler\n",
    "\n",
    "        # FIXME When does this get used\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # FIXME what is happening here\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "\n",
    "def get_embeds(self, input_ids):\n",
    "    return self.word_embedding(input_ids)\n",
    "\n",
    "# FIXME make a real design decision how to use this function\n",
    "def get_logits(self, hidden_repr):\n",
    "   # NOTE we make a simplifying assumption get the logits from linear layer\n",
    "   # FIXME what is a best option for generating logits?? cosine similarity is better?\n",
    "   return self.lm_head(hidden_repr)\n",
    "          \n",
    "\n",
    "  # FIXME what is the difference btween BertModel, BertConfig, BertTokenizer, maybe define it all in one place config for tokenizer + embeddings?\n",
    "\n",
    "\n",
    "def forward(self, x, timesteps):\n",
    "    # FIXME update string comment\n",
    "    \"\"\"\n",
    "    Apply the model to an input batch.\n",
    "\n",
    "    :param x: an [N x C x ...] Tensor of inputs.\n",
    "    :param timesteps: a 1-D batch of timesteps.\n",
    "    :return: an [N x C x ...] Tensor of outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embedded timestep\n",
    "    emb_t = self.time_embed(timestep_embedding(timesteps, self.hidden_t_dim))\n",
    "\n",
    "    # FIXME when would we ever not be the right size? what size will we choose?\n",
    "    if self.input_dims != self.hidden_size:\n",
    "        emb_x = self.input_up_proj(x)\n",
    "    else:\n",
    "        emb_x = x\n",
    "\n",
    "    seq_length = x.size(1)\n",
    "    position_ids = self.position_ids[:, : seq_length ]\n",
    "#     # print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
    "    emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "    emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "\n",
    "    input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "    \n",
    "    if self.output_dims != self.hidden_size:\n",
    "        h = self.output_down_proj(input_trans_hidden_states)\n",
    "    else:\n",
    "        h = input_trans_hidden_states\n",
    "    h = h.type(x.dtype)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noise schedule\n",
    "# TODO consider other noise schedules, here take the simplifying assumption that we use linear noise schedule \n",
    "# Linear schedule from Ho et al, extended to work for any number of\n",
    "# diffusion steps.\n",
    "import numpy as np \n",
    "\n",
    "# TODO make number of diffusion steps a parameter \n",
    "num_diffusion_timesteps = 1000\n",
    "\n",
    "scale = 1000 / num_diffusion_timesteps\n",
    "beta_start = scale * 0.0001\n",
    "beta_end = scale * 0.02\n",
    "betas = np.linspace(\n",
    "    beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    ")\n",
    "\n",
    "# NOTE we assume NO timestep respacing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   diffusion = SpacedDiffusion(\n",
    "#         use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "#         betas=betas,\n",
    "#         rescale_timesteps=rescale_timesteps,\n",
    "#         predict_xstart=predict_xstart,\n",
    "#         learn_sigmas = learn_sigma,\n",
    "#         sigma_small = sigma_small,\n",
    "#         use_kl = use_kl,\n",
    "#         rescale_learned_sigmas=rescale_learned_sigmas\n",
    "#     )\n",
    "\n",
    "#     return model, diffusion\n",
    "\n",
    "# NOTE that diffuSeq uses a gaussian diffusion which can skip steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO explore other simplistic sample code\n",
    "# https://github.com/lucidrains/denoising-diffusion-pytorch\n",
    "# https://e-dorigatti.github.io/math/deep%20learning/2023/06/25/diffusion.html\n",
    "# https://github.com/tanelp/tiny-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GaussianDiffusion model \n",
    "\n",
    "# NOTE adapted from diffuSeq, which is adapted from https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "class GaussianDiffusion():\n",
    "    def __init__(\n",
    "        self,\n",
    "        betas): \n",
    "\n",
    "        # TODO consider adding these other parameters, where are they used, are they required?\n",
    "    #     predict_xstart,\n",
    "    #     rescale_learned_sigmas,\n",
    "    #     learn_sigmas,\n",
    "    #     sigma_small,\n",
    "    #     use_kl,\n",
    "    #     rescale_timesteps=False,\n",
    "    # ):\n",
    "    #     self.rescale_timesteps = rescale_timesteps\n",
    "    #     self.predict_xstart = predict_xstart\n",
    "    #     self.rescale_learned_sigmas = rescale_learned_sigmas\n",
    "    #     self.learn_sigmas = learn_sigmas\n",
    "    #     self.sigma_small = sigma_small\n",
    "    #     self.use_kl = use_kl\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        # FIXME what is alphas? why?\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        # FIXME copied directly\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # log calculation clipped because the posterior variance is 0 at the\n",
    "        # beginning of the diffusion chain.\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    # NOTE the below comments from diffuSeq\n",
    "    # self.mapping_func = None # implement in train main()\n",
    "    # self.add_mask_noise = False # TODO\n",
    "\n",
    "    # FIXME copied directly from diffuSeq\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "    \n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "    \n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "        if mask == None:\n",
    "            return x_t\n",
    "        else:\n",
    "            mask = th.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape)\n",
    "            return th.where(mask==0, x_start, x_t)\n",
    "        \n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.size(0), x.size(-1)\n",
    "        assert t.shape == (B,)\n",
    "        # print(x.shape)\n",
    "        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
    "        \n",
    "        # for fixedlarge, we set the initial (log-)variance like so\n",
    "        # to get a better decoder log likelihood.\n",
    "        model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n",
    "        model_log_variance = np.log(np.append(self.posterior_variance[1], self.betas[1:]))\n",
    "        \n",
    "        model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                # print(denoised_fn)\n",
    "                x = denoised_fn(x, t)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            ### model is used to predict eps\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "            )\n",
    "\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "            top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = th.randn_like(x)\n",
    "            replace_mask = th.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = th.randn_like(noise[replace_mask])\n",
    "                replace_mask = th.abs(noise) > top_p\n",
    "            assert (th.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = th.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"],\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    \n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            if not clamp_first:\n",
    "                if i > clamp_step:\n",
    "                    denoised_fn_cur = None\n",
    "                else:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "            else:\n",
    "                if i >= clamp_step:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    denoised_fn_cur = None\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = th.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        # print(x_start_mean.device, noise.device)\n",
    "        return (\n",
    "             x_start_mean + std * noise\n",
    "        )\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "        # print(logits.shape)\n",
    "        loss_fct = th.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll *= mask\n",
    "        # print(decoder_nll.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1)/mask.sum(dim=-1)\n",
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1)\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t):\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = model_output\n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "\n",
    "        else: # predict eps\n",
    "            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "        \n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "\n",
    "        return {'pred_xprev':pred_prev, 'pred_xstart':pred_xstart}\n",
    "\n",
    "    def training_losses_seq2seq(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs. # not used unless fixing the input embeddings\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        x_start_fix = x_start # save the orignal x_0\n",
    "        assert 'input_ids' in model_kwargs\n",
    "        input_ids_x = model_kwargs.pop('input_ids').to(t.device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask').to(t.device)\n",
    "        x_start_mean = model.model.module.get_embeds(input_ids_x)\n",
    "        \n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   th.tensor([0]).to(x_start_mean.device),\n",
    "                                   x_start_mean.shape)\n",
    "        # print(std.shape, )\n",
    "        # x_start_log_var = 2 * th.log(std)\n",
    "        x_start = self._get_x_start(x_start_mean, std)\n",
    "        # print(x_start_mean.shape, x_start.shape)\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask) # reparametrization trick.\n",
    "\n",
    "        get_logits = model.model.module.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "        assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart'] # predicted_xstart = model_output\n",
    "        t0_mask = (t == 0)\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2)\n",
    "        terms[\"mse\"] = th.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "\n",
    "        # tT_mask = (t == self.num_timesteps - 1)\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, th.LongTensor([self.num_timesteps - 1]).to(x_start.device))\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x) # embedding regularization\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t) # x_0->model_out_x_start\n",
    "        # assert (model.lm_head.weight == model.word_embedding.weight).all()\n",
    "\n",
    "        terms[\"loss\"] = terms[\"mse\"] + decoder_nll + tT_loss\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = th.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n",
    "            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # print(sigma.mean())\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        if langevin_fn:\n",
    "            print(t.shape)\n",
    "            sample=langevin_fn(sample, mean_pred, sigma, self.alphas_cumprod_prev[t[0]], t, x)\n",
    "        \n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_reverse_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n",
    "            - out[\"pred_xstart\"]\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n",
    "\n",
    "        # Equation 12. reversed\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n",
    "            + th.sqrt(1 - alpha_bar_next) * eps\n",
    "        )\n",
    "\n",
    "        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO instantiate the diffusion model & transformer and save it (?)\n",
    "\n",
    "# FIXME some weird comments in diffuSeq, only middle line is uncommented\n",
    "# print('#'*30, 'cuda', dist_util.dev())\n",
    "# model.to(dist_util.dev()) #  DEBUG **\n",
    "# model.cuda() #  DEBUG **\n",
    "\n",
    "# TODO use wandb to save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO instantiate the diffusion model & transformer and save it (?)\n",
    "\n",
    "# FIXME some weird comments in diffuSeq, only middle line is uncommented\n",
    "# print('#'*30, 'cuda', dist_util.dev())\n",
    "# model.to(dist_util.dev()) #  DEBUG **\n",
    "# model.cuda() #  DEBUG **\n",
    "\n",
    "# TODO use wandb to save \n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "# TODO add to logger\n",
    "print(f'### The parameter count is {pytorch_total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE simplifying assumption of uniform sampler\n",
    "class UniformSampler():\n",
    "    \"\"\"\n",
    "    A distribution over timesteps in the diffusion process, intended to reduce\n",
    "    variance of the objective.\n",
    "\n",
    "    Sampler performs unbiased importance sampling, in which the\n",
    "    objective's mean is unchanged.\n",
    "    TODO confirm & update comment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diffusion):\n",
    "        self.diffusion = diffusion\n",
    "        self._weights = np.ones([diffusion.num_timesteps])\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Importance-sample timesteps for a batch.\n",
    "\n",
    "        :param batch_size: the number of timesteps.\n",
    "        :param device: the torch device to save to.\n",
    "        :return: a tuple (timesteps, weights):\n",
    "                 - timesteps: a tensor of timestep indices.\n",
    "                 - weights: a tensor of weights to scale the resulting losses.\n",
    "        \"\"\"\n",
    "        w = self.weights()\n",
    "        p = w / np.sum(w)\n",
    "        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n",
    "        indices = th.from_numpy(indices_np).long().to(device)\n",
    "        weights_np = 1 / (len(p) * p[indices_np])\n",
    "        weights = th.from_numpy(weights_np).float().to(device)\n",
    "        return indices, weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
